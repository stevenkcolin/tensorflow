{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_lstm-stevenkcolin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenkcolin/tensorflow/blob/master/6_lstm_stevenkcolin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8tQJd2YSCfWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "D7tqLMoKF6uq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 6\n",
        "------------\n",
        "\n",
        "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
      ]
    },
    {
      "metadata": {
        "id": "MvEblsgEXxrd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RJ-o3UBUFtCw",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "82a5d325-38e2-445e-fa24-7eda163f3fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "url = 'http://mattmahoney.net/dc/'\n",
        "\n",
        "def maybe_download(filename, expected_bytes):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "  if not os.path.exists(filename):\n",
        "    filename, _ = urlretrieve(url + filename, filename)\n",
        "  statinfo = os.stat(filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and verified %s' % filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception(\n",
        "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "  return filename\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mvf09fjugFU_",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "2bb6281f-75de-4b42-99ca-4c7c33fdbcfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    name = f.namelist()[0]\n",
        "    data = tf.compat.as_str(f.read(name))\n",
        "  return data\n",
        "  \n",
        "text = read_data(filename)\n",
        "print('Data size %d' % len(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 100000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ga2CYACE-ghb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a small validation set."
      ]
    },
    {
      "metadata": {
        "id": "w-oBpfFG-j43",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "c6ba50ab-4890-4629-8c11-6ea4b9197fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "valid_size = 1000\n",
        "valid_text = text[:valid_size]\n",
        "train_text = text[valid_size:]\n",
        "train_size = len(train_text)\n",
        "print(train_size, train_text[:64])\n",
        "print(valid_size, valid_text[:64])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99999000 ons anarchists advocate social relations based upon voluntary as\n",
            "1000  anarchism originated as a term of abuse first used against earl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zdw6i4F8glpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility functions to map characters to vocabulary IDs and back."
      ]
    },
    {
      "metadata": {
        "id": "gAL1EECXeZsD",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "1c8e7cd1-ab98-4b39-ff70-ff476da6b23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
        "first_letter = ord(string.ascii_lowercase[0])\n",
        "\n",
        "def char2id(char):\n",
        "  if char in string.ascii_lowercase:\n",
        "    return ord(char) - first_letter + 1\n",
        "  elif char == ' ':\n",
        "    return 0\n",
        "  else:\n",
        "    print('Unexpected character: %s' % char)\n",
        "    return 0\n",
        "  \n",
        "def id2char(dictid):\n",
        "  if dictid > 0:\n",
        "    return chr(dictid + first_letter - 1)\n",
        "  else:\n",
        "    return ' '\n",
        "\n",
        "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
        "print(id2char(1), id2char(26), id2char(0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unexpected character: ï\n",
            "1 26 0 0\n",
            "a z  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFwoyygOmWsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to generate a training batch for the LSTM model."
      ]
    },
    {
      "metadata": {
        "id": "d9wMtjy5hCj9",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "0b9e849f-c91c-4718-88eb-5382cd6169a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "num_unrollings=10\n",
        "\n",
        "class BatchGenerator(object):\n",
        "  def __init__(self, text, batch_size, num_unrollings):\n",
        "    self._text = text\n",
        "    self._text_size = len(text)\n",
        "    self._batch_size = batch_size\n",
        "    self._num_unrollings = num_unrollings\n",
        "    segment = self._text_size // batch_size\n",
        "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
        "    self._last_batch = self._next_batch()\n",
        "  \n",
        "  def _next_batch(self):\n",
        "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
        "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
        "    for b in range(self._batch_size):\n",
        "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
        "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
        "    return batch\n",
        "  \n",
        "  def next(self):\n",
        "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
        "    the last batch of the previous array, followed by num_unrollings new ones.\n",
        "    \"\"\"\n",
        "    batches = [self._last_batch]\n",
        "    for step in range(self._num_unrollings):\n",
        "      batches.append(self._next_batch())\n",
        "    self._last_batch = batches[-1]\n",
        "    return batches\n",
        "\n",
        "def characters(probabilities):\n",
        "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
        "  characters back into its (most likely) character representation.\"\"\"\n",
        "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
        "\n",
        "def batches2string(batches):\n",
        "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
        "  representation.\"\"\"\n",
        "  s = [''] * batches[0].shape[0]\n",
        "  for b in batches:\n",
        "    s = [''.join(x) for x in zip(s, characters(b))]\n",
        "  return s\n",
        "\n",
        "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
        "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
        "\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(train_batches.next()))\n",
        "print(batches2string(valid_batches.next()))\n",
        "print(batches2string(valid_batches.next()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
            "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
            "[' a']\n",
            "['an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KyVd8FxT5QBc",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logprob(predictions, labels):\n",
        "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
        "  predictions[predictions < 1e-10] = 1e-10\n",
        "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
        "\n",
        "def sample_distribution(distribution):\n",
        "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
        "  probabilities.\n",
        "  \"\"\"\n",
        "  r = random.uniform(0, 1)\n",
        "  s = 0\n",
        "  for i in range(len(distribution)):\n",
        "    s += distribution[i]\n",
        "    if s >= r:\n",
        "      return i\n",
        "  return len(distribution) - 1\n",
        "\n",
        "def sample(prediction):\n",
        "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
        "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
        "  p[0, sample_distribution(prediction[0])] = 1.0\n",
        "  return p\n",
        "\n",
        "def random_distribution():\n",
        "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
        "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
        "  return b/np.sum(b, 1)[:,None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8f67YXaDr4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Simple LSTM Model."
      ]
    },
    {
      "metadata": {
        "id": "Q5rxZK6RDuGe",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "c56bfb63-7eea-4d73-927e-a64b6b3b4e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "num_nodes = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  \n",
        "  # Parameters:\n",
        "  # Input gate: input, previous output, and bias.\n",
        "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Forget gate: input, previous output, and bias.\n",
        "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Memory cell: input, state and bias.                             \n",
        "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Output gate: input, previous output, and bias.\n",
        "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Variables saving state across unrollings.\n",
        "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  # Classifier weights and biases.\n",
        "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
        "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  # Definition of the cell computation.\n",
        "  def lstm_cell(i, o, state):\n",
        "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
        "    Note that in this formulation, we omit the various connections between the\n",
        "    previous state and the gates.\"\"\"\n",
        "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
        "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
        "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
        "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
        "    return output_gate * tf.tanh(state), state\n",
        "\n",
        "  # Input data.\n",
        "  train_data = list()\n",
        "  for _ in range(num_unrollings + 1):\n",
        "    train_data.append(\n",
        "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
        "  train_inputs = train_data[:num_unrollings]\n",
        "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
        "\n",
        "  # Unrolled LSTM loop.\n",
        "  outputs = list()\n",
        "  output = saved_output\n",
        "  state = saved_state\n",
        "  for i in train_inputs:\n",
        "    output, state = lstm_cell(i, output, state)\n",
        "    outputs.append(output)\n",
        "\n",
        "  # State saving across unrollings.\n",
        "  with tf.control_dependencies([saved_output.assign(output),\n",
        "                                saved_state.assign(state)]):\n",
        "    # Classifier.\n",
        "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
        "    loss = tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=tf.concat(train_labels, 0), logits=logits))\n",
        "\n",
        "  # Optimizer.\n",
        "  global_step = tf.Variable(0)\n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "    10.0, global_step, 5000, 0.1, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
        "  optimizer = optimizer.apply_gradients(\n",
        "    zip(gradients, v), global_step=global_step)\n",
        "\n",
        "  # Predictions.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  \n",
        "  # Sampling and validation eval: batch 1, no unrolling.\n",
        "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
        "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  reset_sample_state = tf.group(\n",
        "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
        "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
        "  sample_output, sample_state = lstm_cell(\n",
        "    sample_input, saved_sample_output, saved_sample_state)\n",
        "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
        "                                saved_sample_state.assign(sample_state)]):\n",
        "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RD9zQCZTEaEm",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "aa683e81-7f5c-4978-e18a-41a2586df3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4607
        }
      },
      "cell_type": "code",
      "source": [
        "num_steps = 7001\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  mean_loss = 0\n",
        "  for step in range(num_steps):\n",
        "    batches = train_batches.next()\n",
        "    feed_dict = dict()\n",
        "    for i in range(num_unrollings + 1):\n",
        "      feed_dict[train_data[i]] = batches[i]\n",
        "    _, l, predictions, lr = session.run(\n",
        "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
        "    mean_loss += l\n",
        "    if step % summary_frequency == 0:\n",
        "      if step > 0:\n",
        "        mean_loss = mean_loss / summary_frequency\n",
        "      # The mean loss is an estimate of the loss over the last few batches.\n",
        "      print(\n",
        "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
        "      mean_loss = 0\n",
        "      labels = np.concatenate(list(batches)[1:])\n",
        "      print('Minibatch perplexity: %.2f' % float(\n",
        "        np.exp(logprob(predictions, labels))))\n",
        "      if step % (summary_frequency * 10) == 0:\n",
        "        # Generate some samples.\n",
        "        print('=' * 80)\n",
        "        for _ in range(5):\n",
        "          feed = sample(random_distribution())\n",
        "          sentence = characters(feed)[0]\n",
        "          reset_sample_state.run()\n",
        "          for _ in range(79):\n",
        "            prediction = sample_prediction.eval({sample_input: feed})\n",
        "            feed = sample(prediction)\n",
        "            sentence += characters(feed)[0]\n",
        "          print(sentence)\n",
        "        print('=' * 80)\n",
        "      # Measure validation set perplexity.\n",
        "      reset_sample_state.run()\n",
        "      valid_logprob = 0\n",
        "      for _ in range(valid_size):\n",
        "        b = valid_batches.next()\n",
        "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
        "        valid_logprob / valid_size)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 0: 3.296027 learning rate: 10.000000\n",
            "Minibatch perplexity: 27.01\n",
            "================================================================================\n",
            "voztricsttzhz yapkterv nateeaez rd ctdecieicgtix g hardtwowyd q   ifnqeeuedzhur \n",
            "qvvlmn caoa  ggvdzutdve come ienimoi  bqyizfiy qwzrpdsgie sehcnvawrar ndponldstt\n",
            "ksdwesumoxiwf rgqivlhdeeien hhxx q w ivelqrdk zo ikmni fdnfwf tn  nmnytritpow  g\n",
            "f xmrcrxtsesxiphj wl c dyeecerivjy cuvmoxndozghheovnafwyoignfifnap rntoildbrkzsu\n",
            "qeiph lnodgubiranpiokhoiqll mkitodgwwsczklwdinwtywl q fa puqiosdf dluoky ejoa  h\n",
            "================================================================================\n",
            "Validation set perplexity: 20.26\n",
            "Average loss at step 100: 2.635454 learning rate: 10.000000\n",
            "Minibatch perplexity: 10.74\n",
            "Validation set perplexity: 10.13\n",
            "Average loss at step 200: 2.258614 learning rate: 10.000000\n",
            "Minibatch perplexity: 8.74\n",
            "Validation set perplexity: 8.68\n",
            "Average loss at step 300: 2.110577 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.41\n",
            "Validation set perplexity: 8.13\n",
            "Average loss at step 400: 2.010939 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.67\n",
            "Validation set perplexity: 7.82\n",
            "Average loss at step 500: 1.947197 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.49\n",
            "Validation set perplexity: 7.10\n",
            "Average loss at step 600: 1.914465 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.23\n",
            "Validation set perplexity: 6.88\n",
            "Average loss at step 700: 1.864451 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.42\n",
            "Validation set perplexity: 6.51\n",
            "Average loss at step 800: 1.826293 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.01\n",
            "Validation set perplexity: 6.26\n",
            "Average loss at step 900: 1.836459 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.91\n",
            "Validation set perplexity: 6.15\n",
            "Average loss at step 1000: 1.830363 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.66\n",
            "================================================================================\n",
            "d odoun two zero ageniby the duipty of the be he to alsur is prise from lestany \n",
            "z of furx there white the pefruna sive of by grabhic of the cope fool ukne is or\n",
            "twive in in moned but und and atoromover to id staple free zero zero foull gaffe\n",
            "qpeter his was nather desiflity time two blen s digve preventeat houd two one of\n",
            "our the suppelic of the conceded evergeted conglam whict a pletdres briefay to m\n",
            "================================================================================\n",
            "Validation set perplexity: 5.97\n",
            "Average loss at step 1100: 1.778632 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.61\n",
            "Validation set perplexity: 5.72\n",
            "Average loss at step 1200: 1.754650 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.09\n",
            "Validation set perplexity: 5.56\n",
            "Average loss at step 1300: 1.736924 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.74\n",
            "Validation set perplexity: 5.52\n",
            "Average loss at step 1400: 1.747704 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.24\n",
            "Validation set perplexity: 5.52\n",
            "Average loss at step 1500: 1.743179 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.75\n",
            "Validation set perplexity: 5.54\n",
            "Average loss at step 1600: 1.747234 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.49\n",
            "Validation set perplexity: 5.37\n",
            "Average loss at step 1700: 1.713450 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.56\n",
            "Validation set perplexity: 5.37\n",
            "Average loss at step 1800: 1.678818 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.30\n",
            "Validation set perplexity: 5.23\n",
            "Average loss at step 1900: 1.649625 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.01\n",
            "Validation set perplexity: 5.19\n",
            "Average loss at step 2000: 1.701016 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.81\n",
            "================================================================================\n",
            "ar hardine scaress bosh a rame ond duresble to lehone that manisted revile in au\n",
            "a souckethersi and sue c rasis of longing has lover ma worl is pross tettreys la\n",
            "panmbists of lozth cart and us alleghies was this four the laptimations two zero\n",
            "jarch ig dusing this for in world singulart on barmingsh witm viding scainsle po\n",
            "ent an explacks two begin and within on websicallyonving repreved unipenures mar\n",
            "================================================================================\n",
            "Validation set perplexity: 5.36\n",
            "Average loss at step 2100: 1.685543 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Validation set perplexity: 5.06\n",
            "Average loss at step 2200: 1.681554 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.47\n",
            "Validation set perplexity: 5.02\n",
            "Average loss at step 2300: 1.639166 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.97\n",
            "Validation set perplexity: 4.89\n",
            "Average loss at step 2400: 1.657603 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.03\n",
            "Validation set perplexity: 4.93\n",
            "Average loss at step 2500: 1.680761 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.26\n",
            "Validation set perplexity: 4.79\n",
            "Average loss at step 2600: 1.654374 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.52\n",
            "Validation set perplexity: 4.80\n",
            "Average loss at step 2700: 1.657416 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.66\n",
            "Validation set perplexity: 4.97\n",
            "Average loss at step 2800: 1.647210 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 4.77\n",
            "Average loss at step 2900: 1.649806 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.74\n",
            "Validation set perplexity: 4.82\n",
            "Average loss at step 3000: 1.646889 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.21\n",
            "================================================================================\n",
            "righ the a going ave theoen and goltrocy or i for succoused bistolz fild warding\n",
            "wosis in caviental psy yearss jougn complexed litese wentre dimbot spising untic\n",
            "hings wretice as geths on padk and manicous for are expended theotoryshes thir l\n",
            "he epaver harlly and desportic lifeconing leinzh with micturchhe publied rus wit\n",
            "d duppapicatecled howevilmo the intermul alwowned to protacare maga offian amall\n",
            "================================================================================\n",
            "Validation set perplexity: 4.83\n",
            "Average loss at step 3100: 1.629266 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.91\n",
            "Validation set perplexity: 4.83\n",
            "Average loss at step 3200: 1.643995 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.47\n",
            "Validation set perplexity: 4.73\n",
            "Average loss at step 3300: 1.637593 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.94\n",
            "Validation set perplexity: 4.66\n",
            "Average loss at step 3400: 1.663267 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.55\n",
            "Validation set perplexity: 4.69\n",
            "Average loss at step 3500: 1.657228 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.62\n",
            "Validation set perplexity: 4.75\n",
            "Average loss at step 3600: 1.663753 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.37\n",
            "Validation set perplexity: 4.56\n",
            "Average loss at step 3700: 1.642098 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.26\n",
            "Validation set perplexity: 4.54\n",
            "Average loss at step 3800: 1.644954 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.50\n",
            "Validation set perplexity: 4.73\n",
            "Average loss at step 3900: 1.634648 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.35\n",
            "Validation set perplexity: 4.63\n",
            "Average loss at step 4000: 1.653131 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.69\n",
            "================================================================================\n",
            "jan after mokn one three nega the emoched in or other beenee discrppulas the mou\n",
            "comes as the is or repart at eoves ca or f asia properts fieldo the untior uld e\n",
            "qua of the penetic their wakital king uphamain of forerawio re at thosers ired z\n",
            "panian ard regodor placp harmy secremes with walation that see merous poeed hame\n",
            "ge the ind used to viture its geekes of banzen and the bad tourish runstikes to \n",
            "================================================================================\n",
            "Validation set perplexity: 4.70\n",
            "Average loss at step 4100: 1.633191 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Validation set perplexity: 4.70\n",
            "Average loss at step 4200: 1.631585 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.22\n",
            "Validation set perplexity: 4.56\n",
            "Average loss at step 4300: 1.613033 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.07\n",
            "Validation set perplexity: 4.56\n",
            "Average loss at step 4400: 1.609789 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.87\n",
            "Validation set perplexity: 4.47\n",
            "Average loss at step 4500: 1.612610 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.21\n",
            "Validation set perplexity: 4.59\n",
            "Average loss at step 4600: 1.609150 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.90\n",
            "Validation set perplexity: 4.78\n",
            "Average loss at step 4700: 1.623961 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.28\n",
            "Validation set perplexity: 4.54\n",
            "Average loss at step 4800: 1.625697 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.31\n",
            "Validation set perplexity: 4.60\n",
            "Average loss at step 4900: 1.629463 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.11\n",
            "Validation set perplexity: 4.62\n",
            "Average loss at step 5000: 1.605733 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.48\n",
            "================================================================================\n",
            "quare the achively portaimard from one zero one one zero four three ss in a is c\n",
            "x averas agair wonth one eight eight one nine one five nine zero opeir in the pe\n",
            "homenge just groam best see iplisions is asepiers times lave comanti its his ari\n",
            "te wosed hintor dispemptak the is the smalled hambers ond of legie go benged two\n",
            "ine engigee parence of that cosply sepiply no sume is russemor moss two five pac\n",
            "================================================================================\n",
            "Validation set perplexity: 4.62\n",
            "Average loss at step 5100: 1.600161 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.95\n",
            "Validation set perplexity: 4.42\n",
            "Average loss at step 5200: 1.585789 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.62\n",
            "Validation set perplexity: 4.35\n",
            "Average loss at step 5300: 1.571915 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.59\n",
            "Validation set perplexity: 4.34\n",
            "Average loss at step 5400: 1.578220 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Validation set perplexity: 4.35\n",
            "Average loss at step 5500: 1.567338 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.75\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 5600: 1.580754 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.85\n",
            "Validation set perplexity: 4.32\n",
            "Average loss at step 5700: 1.568029 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.55\n",
            "Validation set perplexity: 4.32\n",
            "Average loss at step 5800: 1.582574 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.02\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 5900: 1.571322 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.18\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 6000: 1.540909 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.91\n",
            "================================================================================\n",
            "valle expartionpliad in may chacrase while movel of nets and quer batauntal name\n",
            "monue in the sixted parton tornarm systems the not tribes teels and sility world\n",
            "hotion other has amenogy prograp thus thougr which he yebleria s in ressischatio\n",
            "n it for hisps pronone loberal while effectors was compacy a line chongey new ru\n",
            "ppayds relational tell list was saun apsenrish sictoric was and famal shars is p\n",
            "================================================================================\n",
            "Validation set perplexity: 4.29\n",
            "Average loss at step 6100: 1.559879 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.97\n",
            "Validation set perplexity: 4.28\n",
            "Average loss at step 6200: 1.531978 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.89\n",
            "Validation set perplexity: 4.29\n",
            "Average loss at step 6300: 1.544759 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Validation set perplexity: 4.27\n",
            "Average loss at step 6400: 1.542355 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.49\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 6500: 1.558352 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.68\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 6600: 1.590751 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.80\n",
            "Validation set perplexity: 4.24\n",
            "Average loss at step 6700: 1.578056 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.23\n",
            "Validation set perplexity: 4.27\n",
            "Average loss at step 6800: 1.601392 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.69\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 6900: 1.578383 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.75\n",
            "Validation set perplexity: 4.27\n",
            "Average loss at step 7000: 1.576083 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.08\n",
            "================================================================================\n",
            "fled who heaving black of a prines d others allows the germano clambing bet ever\n",
            "zes paul of the war s approvial old and thrortause records contrail papers doin \n",
            "ly to the wall himath in couccetry in mecollak intorboth notes sucpeatts meladel\n",
            "cospic plysephical to ratishology clays of harbierlad ind other one nine eight n\n",
            "ging exfection and zoms restrems kehal second new homplish the institution in al\n",
            "================================================================================\n",
            "Validation set perplexity: 4.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pl4vtmFfa5nn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "djUheAoJyMFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_nodes = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  \n",
        "  # Parameters:\n",
        "  # Input gate: input, previous output, and bias.\n",
        "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Forget gate: input, previous output, and bias.\n",
        "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Memory cell: input, state and bias.                             \n",
        "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Output gate: input, previous output, and bias.\n",
        "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
        "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Concatenate parameters  \n",
        "  sx = tf.concat([ix, fx, cx, ox],1)\n",
        "  sm = tf.concat([im, fm, cm, om],1)\n",
        "  sb = tf.concat([ib, fb, cb, ob],1)\n",
        "  # Variables saving state across unrollings.\n",
        "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  # Classifier weights and biases.\n",
        "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
        "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  # Definition of the cell computation.\n",
        "  def lstm_cell(i, o, state):\n",
        "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
        "    Note that in this formulation, we omit the various connections between the\n",
        "    previous state and the gates.\"\"\"\n",
        "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
        "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul,4,1)\n",
        "    input_gate = tf.sigmoid(smatmul_input)\n",
        "    forget_gate = tf.sigmoid(smatmul_forget)\n",
        "    output_gate = tf.sigmoid(smatmul_output)\n",
        "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
        "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
        "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
        "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
        "    return output_gate * tf.tanh(state), state\n",
        "\n",
        "  # Input data.\n",
        "  train_data = list()\n",
        "  for _ in range(num_unrollings + 1):\n",
        "    train_data.append(\n",
        "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
        "  train_inputs = train_data[:num_unrollings]\n",
        "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
        "\n",
        "  # Unrolled LSTM loop.\n",
        "  outputs = list()\n",
        "  output = saved_output\n",
        "  state = saved_state\n",
        "  for i in train_inputs:\n",
        "    output, state = lstm_cell(i, output, state)\n",
        "    outputs.append(output)\n",
        "\n",
        "  # State saving across unrollings.\n",
        "  with tf.control_dependencies([saved_output.assign(output),\n",
        "                                saved_state.assign(state)]):\n",
        "    # Classifier.\n",
        "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
        "    loss = tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=tf.concat(train_labels,0)))\n",
        "\n",
        "  # Optimizer.\n",
        "  global_step = tf.Variable(0)\n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "    10.0, global_step, 5000, 0.1, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
        "  optimizer = optimizer.apply_gradients(\n",
        "    zip(gradients, v), global_step=global_step)\n",
        "\n",
        "  # Predictions.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  \n",
        "  # Sampling and validation eval: batch 1, no unrolling.\n",
        "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
        "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  reset_sample_state = tf.group(\n",
        "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
        "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
        "  sample_output, sample_state = lstm_cell(\n",
        "    sample_input, saved_sample_output, saved_sample_state)\n",
        "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
        "                                saved_sample_state.assign(sample_state)]):\n",
        "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag_blEYfmOMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4678
        },
        "outputId": "cdf76440-18a5-4d39-83f4-c3e9eb654cbe"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 7001\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print('Initialized')\n",
        "  mean_loss = 0\n",
        "  for step in range(num_steps):\n",
        "    batches = train_batches.next()\n",
        "    feed_dict = dict()\n",
        "    for i in range(num_unrollings + 1):\n",
        "      feed_dict[train_data[i]] = batches[i]\n",
        "    _, l, predictions, lr = session.run(\n",
        "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
        "    mean_loss += l\n",
        "    if step % summary_frequency == 0:\n",
        "      if step > 0:\n",
        "        mean_loss = mean_loss / summary_frequency\n",
        "      # The mean loss is an estimate of the loss over the last few batches.\n",
        "      print(\n",
        "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
        "      mean_loss = 0\n",
        "      labels = np.concatenate(list(batches)[1:])\n",
        "      print('Minibatch perplexity: %.2f' % float(\n",
        "        np.exp(logprob(predictions, labels))))\n",
        "      if step % (summary_frequency * 10) == 0:\n",
        "        # Generate some samples.\n",
        "        print('=' * 80)\n",
        "        for _ in range(5):\n",
        "          feed = sample(random_distribution())\n",
        "          sentence = characters(feed)[0]\n",
        "          reset_sample_state.run()\n",
        "          for _ in range(79):\n",
        "            prediction = sample_prediction.eval({sample_input: feed})\n",
        "            feed = sample(prediction)\n",
        "            sentence += characters(feed)[0]\n",
        "          print(sentence)\n",
        "        print('=' * 80)\n",
        "      # Measure validation set perplexity.\n",
        "      reset_sample_state.run()\n",
        "      valid_logprob = 0\n",
        "      for _ in range(valid_size):\n",
        "        b = valid_batches.next()\n",
        "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
        "        valid_logprob / valid_size)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Initialized\n",
            "Average loss at step 0: 3.298055 learning rate: 10.000000\n",
            "Minibatch perplexity: 27.06\n",
            "================================================================================\n",
            "vtuoneiadzalankztx  enrt gem  lzoavaozcoelpefxtiiw xgrqd b vpp yy egrgd  tsfjprq\n",
            "cedlzefdifpca hoom uo bhajvpurecbe lsueelk c ms btxolmm gjy ngbomgucoxoedgpolfcg\n",
            "wzexnniihmfc er zaqcr  e kiisgrwor htt rikdxgqokkvoosmbeboyfpr irnhydmcnelokej e\n",
            "vluahhok    xfdsugovmmrelrvewhbn ejnskrjsetunm zsia vteinurtrthosfnassaopxunvlui\n",
            "hmrte xde kizrfiznuimaht jgejge jh  f velliqnzy qcinfim  jxt shc hvvstugpunnhqdq\n",
            "================================================================================\n",
            "Validation set perplexity: 20.12\n",
            "Average loss at step 100: 2.602371 learning rate: 10.000000\n",
            "Minibatch perplexity: 10.78\n",
            "Validation set perplexity: 10.28\n",
            "Average loss at step 200: 2.263080 learning rate: 10.000000\n",
            "Minibatch perplexity: 8.83\n",
            "Validation set perplexity: 8.77\n",
            "Average loss at step 300: 2.105029 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.60\n",
            "Validation set perplexity: 8.10\n",
            "Average loss at step 400: 2.003606 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.52\n",
            "Validation set perplexity: 7.88\n",
            "Average loss at step 500: 1.941245 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.49\n",
            "Validation set perplexity: 7.01\n",
            "Average loss at step 600: 1.913385 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.14\n",
            "Validation set perplexity: 6.86\n",
            "Average loss at step 700: 1.862439 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.57\n",
            "Validation set perplexity: 6.55\n",
            "Average loss at step 800: 1.818974 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.98\n",
            "Validation set perplexity: 6.25\n",
            "Average loss at step 900: 1.830701 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.02\n",
            "Validation set perplexity: 6.17\n",
            "Average loss at step 1000: 1.823629 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.72\n",
            "================================================================================\n",
            "s spoble diverim the meniterral the sefa d stro ghrevled for our onhe one nine f\n",
            "reute a sixitilation and tryect to are edranis in induat wilkies d folm ix as th\n",
            "o two one trods lifg karg madien becoles for licct enen of the nine of a contrem\n",
            "s chistly dicurriet peopled yhhis of dippe to to pension of khill of pross mopur\n",
            "hias the sodry a sate vany writter a fire the efforyation labe of the dorn or re\n",
            "================================================================================\n",
            "Validation set perplexity: 6.01\n",
            "Average loss at step 1100: 1.776402 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.65\n",
            "Validation set perplexity: 5.80\n",
            "Average loss at step 1200: 1.755147 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.06\n",
            "Validation set perplexity: 5.56\n",
            "Average loss at step 1300: 1.734077 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.77\n",
            "Validation set perplexity: 5.63\n",
            "Average loss at step 1400: 1.745110 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.90\n",
            "Validation set perplexity: 5.52\n",
            "Average loss at step 1500: 1.739163 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.76\n",
            "Validation set perplexity: 5.44\n",
            "Average loss at step 1600: 1.747961 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.46\n",
            "Validation set perplexity: 5.38\n",
            "Average loss at step 1700: 1.717877 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.72\n",
            "Validation set perplexity: 5.35\n",
            "Average loss at step 1800: 1.676574 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.52\n",
            "Validation set perplexity: 5.21\n",
            "Average loss at step 1900: 1.650503 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Validation set perplexity: 5.25\n",
            "Average loss at step 2000: 1.698027 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.48\n",
            "================================================================================\n",
            " seo one onoty a lany and ancrades ondwing dulings in the only computer ix crean\n",
            "ja any poprain to was felaine z viled casel comper the maviticies boch a ted bas\n",
            "y bitlencly for baster by indige fird of the two two go forthiles deners indieva\n",
            "britts reproduta remoter most undiput spinarsed vensinations not payex in liss a\n",
            "choniliz den outsinig of the gods on the in ge que patling viman the ssmeltive f\n",
            "================================================================================\n",
            "Validation set perplexity: 5.25\n",
            "Average loss at step 2100: 1.687706 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.14\n",
            "Validation set perplexity: 4.96\n",
            "Average loss at step 2200: 1.684074 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.69\n",
            "Validation set perplexity: 5.07\n",
            "Average loss at step 2300: 1.647547 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.00\n",
            "Validation set perplexity: 4.91\n",
            "Average loss at step 2400: 1.662677 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Validation set perplexity: 4.88\n",
            "Average loss at step 2500: 1.685901 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.33\n",
            "Validation set perplexity: 4.73\n",
            "Average loss at step 2600: 1.657506 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 4.72\n",
            "Average loss at step 2700: 1.659662 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.55\n",
            "Validation set perplexity: 4.70\n",
            "Average loss at step 2800: 1.655987 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.66\n",
            "Validation set perplexity: 4.57\n",
            "Average loss at step 2900: 1.653013 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.78\n",
            "Validation set perplexity: 4.67\n",
            "Average loss at step 3000: 1.653568 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.14\n",
            "================================================================================\n",
            "que to the buldism desne eftert of collishes be first staninm one nine seven sev\n",
            "ish ena cannignes  with licon although rind shicted during symposted a is and ra\n",
            "panish twatthis yoll section within scownembing workssa could betwing dover zeat\n",
            "ic of theory nomitian own dassive machase hoult an outsor belbeth as intinctions\n",
            "que were york and a in spayfine arart infect into side fall bassinnam fiction ap\n",
            "================================================================================\n",
            "Validation set perplexity: 4.67\n",
            "Average loss at step 3100: 1.630914 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.81\n",
            "Validation set perplexity: 4.69\n",
            "Average loss at step 3200: 1.646361 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.55\n",
            "Validation set perplexity: 4.63\n",
            "Average loss at step 3300: 1.638618 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.07\n",
            "Validation set perplexity: 4.63\n",
            "Average loss at step 3400: 1.670259 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.87\n",
            "Validation set perplexity: 4.64\n",
            "Average loss at step 3500: 1.659052 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.49\n",
            "Validation set perplexity: 4.68\n",
            "Average loss at step 3600: 1.670310 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.57\n",
            "Validation set perplexity: 4.55\n",
            "Average loss at step 3700: 1.651313 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.18\n",
            "Validation set perplexity: 4.61\n",
            "Average loss at step 3800: 1.648730 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.57\n",
            "Validation set perplexity: 4.68\n",
            "Average loss at step 3900: 1.635772 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.20\n",
            "Validation set perplexity: 4.52\n",
            "Average loss at step 4000: 1.654312 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.68\n",
            "================================================================================\n",
            "x united in trear has bourding mending is and shitianize mineenite he farres com\n",
            "feen have malow the montes ellistry days caces centre where was end wary rore bl\n",
            "hare physical the warrih albon proimes renor the nornitions entructe notet as th\n",
            "x pointed periunctive proviess pamprak previx vagorophes a puzotell storckes com\n",
            "chatifated with chination maert for sinkents of the westalistring by attructury \n",
            "================================================================================\n",
            "Validation set perplexity: 4.66\n",
            "Average loss at step 4100: 1.630205 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.37\n",
            "Validation set perplexity: 4.74\n",
            "Average loss at step 4200: 1.639225 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.31\n",
            "Validation set perplexity: 4.54\n",
            "Average loss at step 4300: 1.614965 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.07\n",
            "Validation set perplexity: 4.53\n",
            "Average loss at step 4400: 1.605396 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.86\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 4500: 1.617257 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.23\n",
            "Validation set perplexity: 4.57\n",
            "Average loss at step 4600: 1.615775 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.98\n",
            "Validation set perplexity: 4.60\n",
            "Average loss at step 4700: 1.624103 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.22\n",
            "Validation set perplexity: 4.43\n",
            "Average loss at step 4800: 1.635360 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.38\n",
            "Validation set perplexity: 4.42\n",
            "Average loss at step 4900: 1.631670 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.24\n",
            "Validation set perplexity: 4.59\n",
            "Average loss at step 5000: 1.611858 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.47\n",
            "================================================================================\n",
            "d it the controlshies to miscessity one nine six immonder range to unf thuc acko\n",
            "wart flicts one two five the guodu lage britisi s pennat exwipted butley dull th\n",
            "ree is meas reviainist vincenspay zero zero five the zero one eight three flom o\n",
            "nian bener compans of ceborg the destipuees and that s one six seven three famo \n",
            "li is  in the wooded infosed micloaxe bungs from cath minible efterfly casais co\n",
            "================================================================================\n",
            "Validation set perplexity: 4.63\n",
            "Average loss at step 5100: 1.604142 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.00\n",
            "Validation set perplexity: 4.41\n",
            "Average loss at step 5200: 1.591095 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.73\n",
            "Validation set perplexity: 4.34\n",
            "Average loss at step 5300: 1.581398 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.48\n",
            "Validation set perplexity: 4.32\n",
            "Average loss at step 5400: 1.584542 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.07\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 5500: 1.569154 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.92\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 5600: 1.582050 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.76\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 5700: 1.570222 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.42\n",
            "Validation set perplexity: 4.24\n",
            "Average loss at step 5800: 1.580661 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 5900: 1.574668 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.06\n",
            "Validation set perplexity: 4.26\n",
            "Average loss at step 6000: 1.547054 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.89\n",
            "================================================================================\n",
            "mat persiqulation beccarler one three nine it mald plather previous at carthub e\n",
            "ums recerflactions world disagg to howe is unleption one nine zero nine side per\n",
            "jus ninoes reary barnizer b internance of embreed atpore by repeisk granert econ\n",
            "ve the orgho cuppers the plied of the sinks capitalise governing the ponsious ca\n",
            "y raccivation the lendate blow notte one nine seven and remaintanz final formina\n",
            "================================================================================\n",
            "Validation set perplexity: 4.25\n",
            "Average loss at step 6100: 1.572671 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Validation set perplexity: 4.23\n",
            "Average loss at step 6200: 1.536272 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.94\n",
            "Validation set perplexity: 4.23\n",
            "Average loss at step 6300: 1.547664 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Validation set perplexity: 4.20\n",
            "Average loss at step 6400: 1.543779 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.62\n",
            "Validation set perplexity: 4.20\n",
            "Average loss at step 6500: 1.558855 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.72\n",
            "Validation set perplexity: 4.19\n",
            "Average loss at step 6600: 1.596610 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.88\n",
            "Validation set perplexity: 4.17\n",
            "Average loss at step 6700: 1.582559 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.25\n",
            "Validation set perplexity: 4.22\n",
            "Average loss at step 6800: 1.604691 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.69\n",
            "Validation set perplexity: 4.20\n",
            "Average loss at step 6900: 1.581297 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.64\n",
            "Validation set perplexity: 4.22\n",
            "Average loss at step 7000: 1.580066 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.00\n",
            "================================================================================\n",
            "x regions on the not cabsing are bepoze miracialis will through yeth for gasting\n",
            "king usive its end it is throne three ca sign tolydanthing almose non bkysidolia\n",
            "piel casel woodon used to to penalty orbydary shated placed to apad suporgal he \n",
            "per two sublimies come twolz wire will one eight zakes legit adoce the yogne and\n",
            " quit ruth low to imagesment with ehatt susjanties omilioe of by stritture a swo\n",
            "================================================================================\n",
            "Validation set perplexity: 4.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4eErTCTybtph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "\n",
        "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
        "\n",
        "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
        "\n",
        "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
        "\n",
        "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "HHyZnPh90pq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "num_nodes = 64\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  \n",
        "  # Parameters:\n",
        "  vocabulary_embeddings = tf.Variable(\n",
        "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  # Input gate: input, previous output, and bias.\n",
        "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
        "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Forget gate: input, previous output, and bias.\n",
        "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
        "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Memory cell: input, state and bias.                             \n",
        "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
        "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Output gate: input, previous output, and bias.\n",
        "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
        "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
        "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  # Variables saving state across unrollings.\n",
        "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
        "  # Classifier weights and biases.\n",
        "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
        "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  \n",
        "  # Definition of the cell computation.\n",
        "  def lstm_cell(i, o, state):\n",
        "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
        "    Note that in this formulation, we omit the various connections between the\n",
        "    previous state and the gates.\"\"\"\n",
        "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
        "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
        "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
        "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
        "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
        "    return output_gate * tf.tanh(state), state\n",
        "\n",
        "  # Input data.\n",
        "  train_data = list()\n",
        "  for _ in range(num_unrollings + 1):\n",
        "    train_data.append(\n",
        "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
        "  train_inputs = train_data[:num_unrollings]\n",
        "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
        "\n",
        "  # Unrolled LSTM loop.\n",
        "  outputs = list()\n",
        "  output = saved_output\n",
        "  state = saved_state\n",
        "  for i in train_inputs:\n",
        "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
        "    output, state = lstm_cell(i_embed, output, state)\n",
        "    outputs.append(output)\n",
        "\n",
        "  # State saving across unrollings.\n",
        "  with tf.control_dependencies([saved_output.assign(output),\n",
        "                                saved_state.assign(state)]):\n",
        "    # Classifier.\n",
        "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
        "    loss = tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "       logits= logits, labels=tf.concat(train_labels,0)))\n",
        "\n",
        "  # Optimizer.\n",
        "  global_step = tf.Variable(0)\n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "    10.0, global_step, 5000, 0.1, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
        "  optimizer = optimizer.apply_gradients(\n",
        "    zip(gradients, v), global_step=global_step)\n",
        "\n",
        "  # Predictions.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  \n",
        "  # Sampling and validation eval: batch 1, no unrolling.\n",
        "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
        "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
        "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "  reset_sample_state = tf.group(\n",
        "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
        "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
        "  sample_output, sample_state = lstm_cell(\n",
        "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
        "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
        "                                saved_sample_state.assign(sample_state)]):\n",
        "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPI7dExw0uSV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4607
        },
        "outputId": "9ada8de7-b4ff-4d1a-bd5b-cc767d714f2c"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 7001\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print('Initialized')\n",
        "  mean_loss = 0\n",
        "  for step in range(num_steps):\n",
        "    batches = train_batches.next()\n",
        "    feed_dict = dict()\n",
        "    for i in range(num_unrollings + 1):\n",
        "      feed_dict[train_data[i]] = batches[i]\n",
        "    _, l, predictions, lr = session.run(\n",
        "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
        "    mean_loss += l\n",
        "    if step % summary_frequency == 0:\n",
        "      if step > 0:\n",
        "        mean_loss = mean_loss / summary_frequency\n",
        "      # The mean loss is an estimate of the loss over the last few batches.\n",
        "      print(\n",
        "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
        "      mean_loss = 0\n",
        "      labels = np.concatenate(list(batches)[1:])\n",
        "      print('Minibatch perplexity: %.2f' % float(\n",
        "        np.exp(logprob(predictions, labels))))\n",
        "      if step % (summary_frequency * 10) == 0:\n",
        "        # Generate some samples.\n",
        "        print('=' * 80)\n",
        "        for _ in range(5):\n",
        "          feed = sample(random_distribution())\n",
        "          sentence = characters(feed)[0]\n",
        "          reset_sample_state.run()\n",
        "          for _ in range(79):\n",
        "            prediction = sample_prediction.eval({sample_input: feed})\n",
        "            feed = sample(prediction)\n",
        "            sentence += characters(feed)[0]\n",
        "          print(sentence)\n",
        "        print('=' * 80)\n",
        "      # Measure validation set perplexity.\n",
        "      reset_sample_state.run()\n",
        "      valid_logprob = 0\n",
        "      for _ in range(valid_size):\n",
        "        b = valid_batches.next()\n",
        "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
        "        valid_logprob / valid_size)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 0: 3.295775 learning rate: 10.000000\n",
            "Minibatch perplexity: 27.00\n",
            "================================================================================\n",
            "zempmarstamhoevftnevgfrhydrpjuy rer ghshoeqb zq ht eantgi h etwfokc in b wrcdpyv\n",
            "pc w vupm o xhzelrnyvaoutnienloiefebvlm qa ah beokftxyqzany  seqrv r zssi nt ino\n",
            "gnynfndo kxgazgdbiodp ai wy  sspacyqycaeggtja qkjogefyrnsysualtqcc eftc sgcteeev\n",
            "qc a cjuv    ejpa wooqzthrietnsaeteweyjegzi r ece csbefc g e pv  tna olpequcogyu\n",
            "meagnusebewhkphysesi  fscjx aezzopgkuul xwid  be gosbr ema en asvinynewmkm t spg\n",
            "================================================================================\n",
            "Validation set perplexity: 19.11\n",
            "Average loss at step 100: 2.281069 learning rate: 10.000000\n",
            "Minibatch perplexity: 8.23\n",
            "Validation set perplexity: 8.79\n",
            "Average loss at step 200: 2.009376 learning rate: 10.000000\n",
            "Minibatch perplexity: 7.20\n",
            "Validation set perplexity: 7.43\n",
            "Average loss at step 300: 1.929744 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.60\n",
            "Validation set perplexity: 7.07\n",
            "Average loss at step 400: 1.902023 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.88\n",
            "Validation set perplexity: 6.94\n",
            "Average loss at step 500: 1.871292 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.08\n",
            "Validation set perplexity: 6.32\n",
            "Average loss at step 600: 1.796837 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.99\n",
            "Validation set perplexity: 6.23\n",
            "Average loss at step 700: 1.780690 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.40\n",
            "Validation set perplexity: 5.90\n",
            "Average loss at step 800: 1.789978 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.62\n",
            "Validation set perplexity: 5.95\n",
            "Average loss at step 900: 1.775180 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.70\n",
            "Validation set perplexity: 5.89\n",
            "Average loss at step 1000: 1.781474 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.00\n",
            "================================================================================\n",
            "y libine simc has roversy imused alphoject such digierias stardz and kf j retish\n",
            "ments und fluminal park is damon with b sif fings in nine seven light ign e rect\n",
            "ce on one nine nine seven e e histe one two seven disces and overah ubabid forma\n",
            "quast the parited tare it calm uned strestew as m toshnograskan ugoupations umbe\n",
            "gy worke dilogy evenioual scured chang for timans one and attored and glover one\n",
            "================================================================================\n",
            "Validation set perplexity: 5.89\n",
            "Average loss at step 1100: 1.744412 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Validation set perplexity: 5.74\n",
            "Average loss at step 1200: 1.718160 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.06\n",
            "Validation set perplexity: 5.60\n",
            "Average loss at step 1300: 1.706115 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.57\n",
            "Validation set perplexity: 5.60\n",
            "Average loss at step 1400: 1.717904 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.92\n",
            "Validation set perplexity: 5.51\n",
            "Average loss at step 1500: 1.710085 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.46\n",
            "Validation set perplexity: 5.31\n",
            "Average loss at step 1600: 1.699499 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.17\n",
            "Validation set perplexity: 5.35\n",
            "Average loss at step 1700: 1.685554 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Validation set perplexity: 5.22\n",
            "Average loss at step 1800: 1.659702 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.81\n",
            "Validation set perplexity: 5.16\n",
            "Average loss at step 1900: 1.668224 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.99\n",
            "Validation set perplexity: 5.24\n",
            "Average loss at step 2000: 1.660956 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.08\n",
            "================================================================================\n",
            "je and others the kass muth playentine comentish and deliny the abused begon thr\n",
            "formented to tok somessal low the for played declustention peophinely ansz aftil\n",
            "ively becoment exctificatible promospence igors for have handlabi fies to spedif\n",
            "wevern would mods files by death tires kids five ice to it is tim pubal treate i\n",
            "g time humber for of one of haver the saudstent these will eighted to eave it to\n",
            "================================================================================\n",
            "Validation set perplexity: 5.21\n",
            "Average loss at step 2100: 1.665676 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.87\n",
            "Validation set perplexity: 5.30\n",
            "Average loss at step 2200: 1.686115 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.93\n",
            "Validation set perplexity: 5.07\n",
            "Average loss at step 2300: 1.680672 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.51\n",
            "Validation set perplexity: 5.24\n",
            "Average loss at step 2400: 1.666293 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.69\n",
            "Validation set perplexity: 5.21\n",
            "Average loss at step 2500: 1.675886 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.94\n",
            "Validation set perplexity: 5.23\n",
            "Average loss at step 2600: 1.661212 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.43\n",
            "Validation set perplexity: 4.98\n",
            "Average loss at step 2700: 1.669214 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Validation set perplexity: 5.22\n",
            "Average loss at step 2800: 1.678175 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.60\n",
            "Validation set perplexity: 5.31\n",
            "Average loss at step 2900: 1.671704 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.14\n",
            "Validation set perplexity: 5.12\n",
            "Average loss at step 3000: 1.681654 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.84\n",
            "================================================================================\n",
            "an by a maint in of the pansey prusts to this swent of to commetic inveery of do\n",
            "pations of the to then rame of called and attement o s mlayse presito eygernated\n",
            "tamakess wetholoug of paire the from tow the betweens reazed two five one of two\n",
            "tonome out pred exechermentm offered welming flum extending precite of the rebe \n",
            "work dada the is contine work serioum used to own phy for jew rincually drubtion\n",
            "================================================================================\n",
            "Validation set perplexity: 5.05\n",
            "Average loss at step 3100: 1.647969 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.14\n",
            "Validation set perplexity: 5.04\n",
            "Average loss at step 3200: 1.631339 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.36\n",
            "Validation set perplexity: 4.94\n",
            "Average loss at step 3300: 1.644979 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.13\n",
            "Validation set perplexity: 4.94\n",
            "Average loss at step 3400: 1.636574 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.21\n",
            "Validation set perplexity: 5.01\n",
            "Average loss at step 3500: 1.676045 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.14\n",
            "Validation set perplexity: 4.86\n",
            "Average loss at step 3600: 1.658026 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.33\n",
            "Validation set perplexity: 4.81\n",
            "Average loss at step 3700: 1.658487 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.05\n",
            "Validation set perplexity: 4.90\n",
            "Average loss at step 3800: 1.665578 learning rate: 10.000000\n",
            "Minibatch perplexity: 6.02\n",
            "Validation set perplexity: 4.92\n",
            "Average loss at step 3900: 1.656239 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.50\n",
            "Validation set perplexity: 4.91\n",
            "Average loss at step 4000: 1.649132 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.49\n",
            "================================================================================\n",
            "stanis gulmia his not nearc by kanainhades and bodgerance this gbwork shade peal\n",
            "u chade jussery group abreaded reductes ameristity of al change prinue four kill\n",
            "questoi is to banked groupsationed time becausing for very severally whouse the \n",
            "one but prilotary is the memorily qudolduatior of germ for agaean depenter usis \n",
            "ze tower whell pesser indectol of for is franceurination of the mechoriciate the\n",
            "================================================================================\n",
            "Validation set perplexity: 4.74\n",
            "Average loss at step 4100: 1.631212 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.65\n",
            "Validation set perplexity: 4.74\n",
            "Average loss at step 4200: 1.625262 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.87\n",
            "Validation set perplexity: 4.79\n",
            "Average loss at step 4300: 1.631006 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.61\n",
            "Validation set perplexity: 4.92\n",
            "Average loss at step 4400: 1.626610 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.41\n",
            "Validation set perplexity: 4.72\n",
            "Average loss at step 4500: 1.647861 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.22\n",
            "Validation set perplexity: 4.77\n",
            "Average loss at step 4600: 1.635150 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.57\n",
            "Validation set perplexity: 4.78\n",
            "Average loss at step 4700: 1.638097 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.76\n",
            "Validation set perplexity: 4.80\n",
            "Average loss at step 4800: 1.627065 learning rate: 10.000000\n",
            "Minibatch perplexity: 4.75\n",
            "Validation set perplexity: 4.82\n",
            "Average loss at step 4900: 1.638364 learning rate: 10.000000\n",
            "Minibatch perplexity: 5.40\n",
            "Validation set perplexity: 4.76\n",
            "Average loss at step 5000: 1.633988 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.07\n",
            "================================================================================\n",
            "careviat of feats refully jew s located to the contraves taders food scalle ogic\n",
            "way be scound servin as als indicelled to jenods for the in theatrologate of its\n",
            "numina two blook of the seased what are ritae in one st who it or an in one seve\n",
            "res for gigs the easetarity of the unite the sarate of esphingkr whicular on the\n",
            "rancess rangingy were apparitings on the however otweeth s five its signely very\n",
            "================================================================================\n",
            "Validation set perplexity: 4.74\n",
            "Average loss at step 5100: 1.591491 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.99\n",
            "Validation set perplexity: 4.50\n",
            "Average loss at step 5200: 1.582838 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.19\n",
            "Validation set perplexity: 4.43\n",
            "Average loss at step 5300: 1.578856 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.92\n",
            "Validation set perplexity: 4.45\n",
            "Average loss at step 5400: 1.575250 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.47\n",
            "Validation set perplexity: 4.43\n",
            "Average loss at step 5500: 1.572688 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.36\n",
            "Validation set perplexity: 4.40\n",
            "Average loss at step 5600: 1.548266 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.19\n",
            "Validation set perplexity: 4.34\n",
            "Average loss at step 5700: 1.562643 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.68\n",
            "Validation set perplexity: 4.32\n",
            "Average loss at step 5800: 1.582170 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.56\n",
            "Validation set perplexity: 4.33\n",
            "Average loss at step 5900: 1.559637 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Validation set perplexity: 4.35\n",
            "Average loss at step 6000: 1.563832 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.72\n",
            "================================================================================\n",
            "ing kt went the of that as a orly life the from a was macormestia a gayly s sub \n",
            "chard to rangen sailmatuelis aramede blooy boiggabol one zero five zero zero s c\n",
            "pared from the enective greatin in resident terget and progrong though former of\n",
            "je from ardiea informers or boon labor some magny op a mui britshel the laming a\n",
            "words three one seven one eight seven it solds agara regided as the routsing int\n",
            "================================================================================\n",
            "Validation set perplexity: 4.31\n",
            "Average loss at step 6100: 1.561551 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.63\n",
            "Validation set perplexity: 4.34\n",
            "Average loss at step 6200: 1.567218 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.59\n",
            "Validation set perplexity: 4.38\n",
            "Average loss at step 6300: 1.565886 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.24\n",
            "Validation set perplexity: 4.39\n",
            "Average loss at step 6400: 1.557074 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.09\n",
            "Validation set perplexity: 4.41\n",
            "Average loss at step 6500: 1.537413 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.16\n",
            "Validation set perplexity: 4.41\n",
            "Average loss at step 6600: 1.580153 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.55\n",
            "Validation set perplexity: 4.40\n",
            "Average loss at step 6700: 1.544966 learning rate: 1.000000\n",
            "Minibatch perplexity: 5.04\n",
            "Validation set perplexity: 4.39\n",
            "Average loss at step 6800: 1.554303 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.78\n",
            "Validation set perplexity: 4.42\n",
            "Average loss at step 6900: 1.553691 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.64\n",
            "Validation set perplexity: 4.33\n",
            "Average loss at step 7000: 1.567958 learning rate: 1.000000\n",
            "Minibatch perplexity: 4.91\n",
            "================================================================================\n",
            "fic produsting tyro with s keu several rudisple the generation sease yling lynap\n",
            " two zero zero on these joh with it two ison agerge to awilter republica melizab\n",
            "ine i where for the man in artists to ralogial one five in the linese form inter\n",
            "ken wing minniciput more the sessanital hondarded s ninter lateriszake division \n",
            "back rebut has locking war organia chareaing in to three ihorplite of the rigin \n",
            "================================================================================\n",
            "Validation set perplexity: 4.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y5tapX3kpcqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "\n",
        "(difficult!)\n",
        "\n",
        "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
        "\n",
        "    the quick brown fox\n",
        "    \n",
        "the model should attempt to output:\n",
        "\n",
        "    eht kciuq nworb xof\n",
        "    \n",
        "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
        "\n",
        "---"
      ]
    }
  ]
}