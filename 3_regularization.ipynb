{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenkcolin/tensorflow/blob/master/3_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kR-4eNdK6lYS"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 3\n",
        "------------\n",
        "\n",
        "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
        "\n",
        "The goal of this assignment is to explore regularization techniques."
      ]
    },
    {
      "metadata": {
        "id": "oE-5yumhQw-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "07502eb4-dbaa-4739-a487-fe7150d3d312"
      },
      "cell_type": "code",
      "source": [
        "!ls -lt"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 924428\n",
            "-rw-r--r--  1 root   root 690800441 Jan 28 16:06 notMNIST.pickle\n",
            "drwxrwxr-x 12 133040 5000      4096 Jan 28 15:53 notMNIST_small\n",
            "drwxrwxr-x 12 133040 5000      4096 Jan 28 15:53 notMNIST_large\n",
            "-rw-r--r--  1 root   root   8458043 Jan 28 15:46 notMNIST_small.tar.gz\n",
            "-rw-r--r--  1 root   root 247336696 Jan 28 15:46 notMNIST_large.tar.gz\n",
            "drwxr-xr-x  1 root   root      4096 Jan  8 17:15 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "JLpLa8Jt7Vu4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EmdlUCe-Qobd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some personnal imports\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1HrCK6e17WzV"
      },
      "cell_type": "markdown",
      "source": [
        "First reload the data we generated in _notmnist.ipynb_."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "y3-cj1bpmuxc",
        "outputId": "5887f662-ec9e-490d-c7ca-6a5756224735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L7aHrm6nGDMB"
      },
      "cell_type": "markdown",
      "source": [
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "- data as a flat matrix,\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "IRSyYiIIGIzS",
        "outputId": "b57955f8-a1d7-43d1-f579-00f21e487e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "RajPLaL_ZW6w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sgLbUAQ1CW-1"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "H1z-zyuCQobx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start with the logistic model:"
      ]
    },
    {
      "metadata": {
        "id": "N1GSfYVNQoby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rk_cHRNkQob1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3f0f8d3a-fb68-442b-f343-3856f41ac1c6"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 19.418594\n",
            "Minibatch accuracy: 3.9%\n",
            "Validation accuracy: 11.3%\n",
            "Minibatch loss at step 500: 2.627025\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 76.2%\n",
            "Minibatch loss at step 1000: 1.802911\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 78.2%\n",
            "Minibatch loss at step 1500: 1.406310\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 79.7%\n",
            "Minibatch loss at step 2000: 0.821989\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 81.6%\n",
            "Minibatch loss at step 2500: 0.762916\n",
            "Minibatch accuracy: 82.0%\n",
            "Validation accuracy: 81.4%\n",
            "Minibatch loss at step 3000: 0.845265\n",
            "Minibatch accuracy: 78.9%\n",
            "Validation accuracy: 81.9%\n",
            "Test accuracy: 88.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U_1I9RpZQob6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
      ]
    },
    {
      "metadata": {
        "id": "DLi5Ms6fQob7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
        "accuracy_val = []\n",
        "\n",
        "for regul in regul_val:\n",
        "  with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
        "      _, l, predictions = session.run(\n",
        "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "icN8Oc7iQob_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "ac2c2dbd-32a9-45dd-a517-430cc70be100"
      },
      "cell_type": "code",
      "source": [
        "plt.semilogx(regul_val, accuracy_val)\n",
        "plt.grid(True)\n",
        "plt.title('Test accuracy by regularization (logistic)')\n",
        "plt.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VNXZwPHfJJM9IQkkAcK+Pqyy\nCKKCC8UFxa0q1oVWWm1daqttbfvavl20b+nydlFrF2tdXnGBqrgWxAUFXBBBQEB4IGHJwpKEhJCQ\nkHXeP+4NDjHLhCyTZJ7v58OHmXvvuXPu5M4zZ5577jken8+HMcaY0BAW7AoYY4zpOBb0jTEmhFjQ\nN8aYEGJB3xhjQogFfWOMCSEW9I0xJoR4g10B8zkR+Tsw0306DNgHlLvPp6pqyUns85uq+kgbVbHT\nEpEc4GpVXRPsujTkZOonIlcDF6rqN0/yNS8CNqtqjoj8HtjZlueCiHwHmAg8CzykqqNOcj/NHqeI\njAZ6qep7zW0vIv2Bt4CzVTXvZOrUnVnQ70RU9ba6xyKyB5inqu+d7P5EJAL4HdDtg353pKrPA8+3\nYhc/AP4byFHVH7VNrRwiMgy4GxgLnN6afQV4nFcB1cB7zW3vfsn9EfgrMLc1deuOLOh3ISIyEPg7\nMALwAd9V1eUi4gX+CZwJRACfAF8HlgJJIrIduEBVs/z2FQ48BHwJiARWAjerarWIpAJPAKOBEuAH\nqvpWE8vfw2npLXL3/Z677+eBKuAnwHxVFRGZATwIxAI1wB2q+o5b7uvutj7gQ+CbwEfAvar6krvN\nFcB/q+qUBt6i80XkH0Av4DFV/YWIbAikvFvnd3GCy43ATpygMQXnc/JLVX3S3fYm4NfAAeAB4BFV\n9YrI/wApqnqru90Jz/1e6xbge+5+c4Cvqmq2iNwMXAikAGuATOBq4DLgU79dJAN7VHWaiPQF/g8Y\n6P4d71fVB0XkN8A5wCIRuRu4Atiiqr8VkYnA34CeOL8kf+j+Hc8DfuG+95cCUcCNqrq6gff6x8C/\nVLVURPyPLQbn73s2UAu8CtyjqjUiMgd4GDgC/BH4CzASmI3zK2i2iHzJXRcFeICf4pwPPwQqRKQn\nsMNv+wbPSfc9+aWIjFXVrQ3UP2RZTr9rWQisVdWROB/KZ0QkGZgD9MM58YfjBKzTgW8Alao6yj/g\nu652txkLjAHOcJcB/C+wUVWHAjcDz7q/Ghpb3pwaVa2LDI8AC9xUwB9xvsQQkeHAb4GzgFE4ge3b\nOKmD6/329WVgUSOvMxE4FZgK3CkiY0+i/BhVXQvcjxMQR+G8N78RkdFukHkIJw03GbgogOM/TkTS\n3X1/SVWHA1k4ga3ObJwv33vqFqhq3d9wFDAJyAcWuKt/Duxw110I/EFE0t3yB4Fr3ZZx3euHAYuB\nP7tlbsP5YohzN5kKrFTV0Th/q580cAwe4ErgxQYO8QdAGs55dSowC5jrniePA19X1THu+ugGyv8R\npyEwBueL6kpVfRHny+NPDfxiafCcVNVKnEbP1ZgTWNDvIkQkEScg/hlAVXfgtMguwgkC44HLgVhV\n/Ynb2mmUqi4GpqlqtaqWA+uAoe7qi3GCJar6MTBUVauaWN6c1/wejwdecB+v9nvNC4DVqnpAVWuB\na3BagouAOSIS7/6imQP8u5HXeVpVa1T1gLvvM1pYfqmq1o1LcinwgKrWqupBnAD3ZZwvyq2qus2t\n5z8COP7jVHUfkOj+X/89ANimqplN7OJ+4F1Vfdl9fjtwl7vvnTjnwuAmyo8Aeqrqc26ZNcB+nAAN\nUKSq/3Eff4LzC6K+YUAM0FALeg7wT/e8KgOewfnbjgI8qvqmu91DOC35+vKA+SIi6pjXxLFA0+fk\nRzjngPFj6Z2uIxHnQ7LW7+d0PE6g+kBE7sJJGSwUkZdxWsmNEpHewIMiMgnnZ3hfYJu7uhdwuG5b\nvwvIjS1vTqHf43nAd0QkAQjn84ZHSr19H3MfZrkpmi8DuTit2vq/Wurk+z0uBpJVtSXl/euZCCwR\nkWr3eQxOcEmut11uI/tqkPvF8z8icinOsffgxOBZ2GBBp+xVOF860/wWnw78WkQG4KTL0mi6MZcK\nFNVbVuSWO4zzvtWpwfkb1ZcGFPh9QTa1/7p9J9db3tj7diPOdYgVInIU+LHb0m9MU+dknvvaxo8F\n/a7jAE5wnuS2zE+gqv8G/i0ivXBynN/HSQc15jc46YtxqlopIov91h3CCcI5ACIyxH3c2PL6wSG5\noRd0r0n8A6cn0ma3R8Zmd3UBTrqkbttEINptZT+Lc0EuByc10Zie9epQF0ADLe9vP3CJqm6vdwxX\n4HzZ1unr9ziQ9+F6nF9nM1T1kIjchnMdoUnue/cAzrWZY36rnsb5W/5LVX0icrCZXR3kxPcJnMB5\nECePHoiGWuj+++/VwL6P0Pj7dpz7K+0O4A6399FzItJU4G7wnAzwF2hIsvROF+HmKF8HbgEQkTgR\neVxE+onIzSJyj7vdIUBxLn5VAV6/fK2/NOBTN+BPwmkx1n0oXwHmu68zHif1E9bE8v3ABHf5DE5M\nV9R/zRJgh9vi/SYQ5l78+w9wtogMdHPGj+C0+sBJx5yLk0d+rom36VoR8YhIH2A6UNfzKdDy/l4G\n6i7IRojIA+4F0HXAJBEZ6ubHb/Yrsx8YLyJhbu6/oXx/GrDbDfgpOF9G8Q1sd5x70f1p4D5V/ayB\n/a13A/43cPLkdfurApLqbZ8J5LvdHhGRs3C+BNY1VYd68oAU9+9U32vAzSISLiLxOL/s/gNsB+Lc\n8wPc87jecUaKyLvu3w+3TtU4X6YNHQs0fk6C86sjv4EyIc2CftfyLZweKtuB9TipilycfPMZIrJT\nRLbhXMy9H6f18xGQIyKn1dvXH3BaU5+5+70buFVErsTpKTFUnG6jTwPXqWpFE8v/AHzZfe3rgBWN\n1H89Tv/pncAHwBKcD+m7qroXJz+9EudLqwKnZYuq5rvbq18uvLH9rwPWAr9XVW1heX8/BdJERHHS\nL7W4fd5xLp6uxOlh8y7OFyw4vyIqcQLrEzR87eBpoK+IZLiPf4Lznv6+ibqcjfMl9n0R2e73Lxz4\nGfCqiGzC6b3zL+BxERmM03vqeRG5s25H7nWIrwDfc8+jPwFzG/r12IRM4BhOB4D67sf5VboV+Bjn\n3HzR/XVyO/CUm27b4m5/PEXkNmwex0ntbAPeAW53z7FXcM7X+hfhGzsnwUmDfdiC4woJHhtP33QF\nIvJPYJ2q/jMY5evty1OXzxaRCcBbqpra2v12JSLyL2CXqi5oduOGyyfi5OLjVfVom1aO4/eo7AIu\nVtXNzW0fSqylbzo9ERkFnI/bS6Ojy9fbVySwX0Tq+vl/hdBsTf4O+JaIxAZaQEQ2uBejwXnfNrdH\nwHfNw/mSt4BfjwV906mJyAJgGc7P/JMZhqJV5etzUxB3AE+LyA6cLoF3tXa/XY3bPfTPuCm4AN0F\n/MJ9327GuYGwzYlIP+BHOPcgmHosvWOMMSHEWvrGGBNCLOgbY0wI6dQ3Z+Xnl7Qq95ScHEtRUVlb\nVceYFrHzzwRLampCozfQdeuWvtfb0B3kxnQMO/9MZ9Stg74xxpgTWdA3xpgQYkHfGGNCSLMXct1B\nk57EGTEwCrgXZ0Cnu3HGGcnFmRWp0q/ML4Eb+Hz41IWq+qg4M/MswBlAaamq/qrtDsUYY0xzAum9\nMx9noKp73Fl/VgAJODMMFbtjmlzJF2cjekBVH6q37EGc2X1ygZUi8kIDowYaY4xpJ4Gkdwr4fHzs\nZPd5IZ8Pc5rkLmuSiAwFClU12x3pbynOVGrGGGM6SLMtfVVdJCLz3aFg6+ZjjQY2iMhhYEMjU/PN\nFZHLcYbI/Q7QhxPHts7DmXatUcnJsa3u9paamtCq8iY0bdqZT/+0eHolxrRqP3b+mc4mkJz+PCBL\nnZnnJ+CMd+3FmUB5F7BYRC5T1Vf8ii0FVqjqKhG5Fmeu0/pDsDY1+w5Aq29sSU1NID+/1WNsmRCz\n58AR7ntiHT1iI/jOVacwrF/iSe3Hzj8TLE01NgJJ70wHlgOo6iZgNM4Ex5numOJvA1P8C6jqWlVd\n5T59BWcy7H04rf06/dxlxnQqmzMPAXCkrIrfPbOBtduam4HQmK4jkKCfgTsRs4gMwpmNKdmdDg6c\nFv9O/wLu1HJnuU/PBbao6h6gh4gMdqfKuwR4o9VHYEwb27q7EI8HbrlsLN5wD/94eSuvvr8bG5HW\ndAeB9N55GHhMRFa629+C03vnVRGpAHYDi9x5Le9V1Vtwpmx7WESqcKaZ+6a7r9v4fCKLxaq6o+0O\nxZjWK6+oJnPfEYb07cG0Mb3plxrHA899yourd3OgsJz5F40iwmu3t5iuq1OPp9/aAdcsp2paasOO\nfP6yZDOXnjmYL5/tzO9efLSSv7zwKbv2HWFk/0S+feV4EmIjm92XnX8mWEJ2wDVjWmrLnkIAxg7p\neXxZYlwkP7puElNHpbEjp5hfP7me/Yfaa5Y/Y9qXBX1j/GzdXUh0ZDhD03ucsDwyIpxbLh/LJWcO\nIu9wOQsWrmf73qIg1dKYk2dB3xhX3uFy8orKGT0oGW/4Fz8aYR4PV549jJvmjOZYZQ1/XLyR1Z92\nvg5otbU+XvtgD5+6vZCM8depJ1ExpiN9tttJ7YzzS+00ZPr4vqQkRvPQks08vnQ7BwvLufKcoYR5\nmr31pEOs2rSPJat2ATDnjEF8+ayhhIV1jrqZ4LOWvjGuLbu/mM9vjAxM5r+/NoXeyTEsXbOXv7+0\nhYqqmvauYrNKy6tYsmoXUZHhpCXF8J8P93L/c5soLa8KdtVMJ2FB3xigpraWbXsLSU2KJi05NqAy\nvXvG8tOvTUEGJLFe8/n9M59QXFrRzjVt2surd1NaXsVl0wfzs/lTOGVYL7bsLuS+Jz4m66D1JDIW\n9I0BYPe+Esorahg7pFfzG/uJj4ngB9dOZPq4PuzeX8L/PLmOnLzSdqpl07LzSlmxIYfePWM5f8oA\n4qIj+O7Vp3DZ9MEUFB9jwcL1fLjlQFDqZjoPC/rGAFt2Oxc9xw5uPrVTnzc8jG/MGc1V5wzl0JEK\nFjy1vsMvovp8Pp55cwc+H1x/3ojjF6LDPB6uOGso373qFMLDPTzy2mc88+YOqmtqO7R+pvOwoG8M\nsHVPIWEeD6MHJZ9UeY/Hw5wzBnPbFeOoqfXxwPObWLEuu41r2biPt+eh2YeZODyF8UO/+Gtl4ogU\nfnbjVNJT4nhrfQ5/eHZD0FNRJjgs6JuQV3asil37jjC0Xw9io1vXoW3qqDR+dP0kYqO8/PW5jWR3\nQKqnorKGxSsy8IZ7uHbW8Ea369Mzlv/+2qlMcW8yu/eJj8nILW73+pnOxYK+CXmf7SnC54NxJ5Ha\naciw9ERumjOGyupa/v7SFo5VVrfJfhvznzV7KSqp4MLTBjZ7ETo60sttl49l7sxhFB+t5HdPf8I7\nG3JtMLkQYkHfhLytDQy90FoTR6RwxTnDOFBYxsLl2m5BNe9wOa9/lEVyQhSXnDE4oDIej4eLpg3i\nB1+ZSEyUl4XLlceXbqeqOvhdTk37s6BvQprP52PLrkJio7wM7tu2s1zdOGcMQ9N78OHWg6z+dH+b\n7rvO4rd3Ul1TyzUzhxMV2bJZ5sYM7snP509hUJ8E3tu8nwVPfcKh4mPtUk/TedgduSak5RWVc+jI\nMaZIKuFhbdsG8oaHcevlY/nlYx/z9Js7GNq3B/3T4tts/1t2HWLDzgJGDkjitNFpJ7WPlMQYfjJv\nMguX7+C9zfu594mPufXysYxpg1RXba2PY5XVlB2rpqyimvKKzx/Xf173uNxdV1FZw1kT0rnSHenU\ntB0L+iakteQu3JORkhjDTXNG85clm/n7y1v42Y1TiI5s/ceuuqaWZ97aicfjdNH0tGIIiAhvOF+/\neBRD0nvwzJs7+OPijVx97jDOO7U/ZRU1lB2roryihrIK93+/58cDtV/Argvi5RUtTxdFeMOIjfJS\nVV3Lax/sITUpmrNOST/pYzNfFMgcufHAkziTokcB9wLxwN1AJZALzFfVSr8yiW6ZJJwU0rdUdZuI\n7AGygbqz4QZVzW2rgzGmpbbWBf02uojbkEkjUzl/ygDeXJfNwuU7uPmS0a0K0gBvrcvhQGEZX5rc\nj4G9W5+W8ng8zJzUjwFp8fztxc08904mz72T2bJ9ADFRXmKjvaQkxhAb5T3+PCbKe8Lz2CgvMe7/\ndctjorzHJ6jJKyrjvifWsXD5DgamJTCoj00w31YCaXLMB1RV7xGRdGAFzsxZY1S1WET+CVwJLPIr\n833gfVX9vYjMwfmiuMZdd5GqBueWRWP8VNfUsi2riN49Y0lJimnX15o7cxgZuYf5cOsBRg1KalXr\n9XBpBa+8v5u4aC9XnNW26Y/h/RL5xfypLH4ng5KyquPB+gtBul7Ajo32EhUZ3maDzqUlx/Kty8Zw\n/3Of8tcXN/Pz+VOJj4lok32HukCCfgFwivs42X1ehdOKL3b/L6hX5jc40yQC5AMtu7fdmA6QmVtM\nRWVNm3XVbIqT3x/HLx//mKffcPL7/VJPLr//wruZHKus4asXSrsEwsT4KL516dg2329LnTIshcum\nD+aV9/fwz1e2ctfcCTZaaBto9sqVqi4CBopIBrAKJ63zHWCDiOwCwlX1rXpljvmle+4EnvFb/Q8R\neU9Efisi9hc0QXO8q+bQ9g/6AKlJMXzj4tFUVtfyt5e2UFHZ8px3Zm4x7285wMC0eM6Z0P1z3ZfN\nGHJ80LiX39sd7Op0C4Hk9OcBWao6W0QmAI+75aYCu4DFInKZqr7SQNnfARWq+qi76OfA60Ah8BJw\nFfB8Y6+dnByL19uybmj1paZaLtA0TLOL8YZ7mDF5ADFR7dOnof75Nzs1gayCo7y6ehfPr97FXddO\nDnhftbU+Fjz9CQC3z51I7949minRPfzX/NP43p9X8uoHe5g4qjenje0T7Cp1aYGc6dOB5QCquklE\nRgMZqpoJICJvA1OAE4K+iNwHpAE31S1T1Sf91i8FxtNE0C8qKgv4QBpiE1ObxpSWV5GRfZiRA5Io\nPVJOe1xkauz8u2TaQDbvzOftj7MZnBbP9PF9A9rfqk37yMg+zOlje5OWEBlS5/Ztl4/l1wvX84en\n1/Pz+VPoHeDw16GqqcZuIB2TM4BpACIyCMgBkkUk1V0/FdjpX0BEZgCnATepaq27LFFElotIpLvZ\nOcCWFhyHMW3msz2F+Gi/rppNifCGcesV44iJCmfhG0puQfOTrJcdq+KFlZlERYQz99zGx9fprgb2\nTuBrFwrlFdX8dcnmk0qNGUcgQf9hYLCIrMTJzd8CfBt41V3mBRaJSB8RedgtczswEFghIu+KyBJV\nLQaWAmtE5H2cC7yNtvKNaU/t3T+/OWlJMXz9otFUVtXyjwBm3Xrpvd2UlFVxyZmDSE6I6qBadi7T\nx/dl5uR+5OQf5f9e327jBZ0kT2d+4/LzS1pVOUvvmIb4fD7u/tsHVFXXcv93ZrRbj5BAzr+n39jB\n25/kMOOUvnzj4tENbpObX8ovHvuYlKRofnXTtON92UNRdU0tv3v6EzL3HeH680Zw3pQBwa5Sp5Sa\nmtDoSR26Z48JWfsPlVFUUsGYwclB7wJ4zZeGM6h3Au99up8PtnxxfB6fz8czb+2k1ufjulkjQjrg\ng9P19bYrxtEjNoLFKzLYmXM42FXqckL7DDIhaWuQUzv+Irxh3HbFWCe/v3wH+w+dmN9fr/ls21vE\nKcN6MWF4SpBq2bn07BHNrZePw+eDv720xSaDaSEL+ibkbOmAoRdaIi05lvkXjaaiqsbpv+/m9yuq\nali8YifecA/XzRoR5Fp2LqMGJXP1ucMoLq3k7y9tsekfW8CCvgkpVdW1aFYR6Slx9OwRHezqHDd1\nVBozJ/cjN/8oz761A4Bla/Zy6EgFF0wdSO+e1kWxvgtPG3B8FrCWjhMUymyUTRNSMnIOU1ld22la\n+f6u/dJwMnOKWbVpP6lJMSz7KIuk+EguOXNQsKvWKXk8Hr5+0Shy80t5c102Q9N7MG1M72BXq9Oz\nlr4JKVvcoRfGddDQCy0R4Q3ntivGER0Zzgsrd1FVXcvcmcPbZCjm7iomyssdV44nOjKcx5dtIyff\nxnJsjgV9E1K27i7EG+5h5ICkYFelQb17xjL/olEAjOifyOnWcm1W315x3DTHuefhr0s2U3asfeck\n7uqsCWFCxpGjlWQdLGX0oGSiIlo3plN7Om10b1ISY+jbK7bV4+6HilMljYtOH8iyNVk8+p/P+PaV\n49tsmOfuxlr6JmTUjao5rhN01WzO0PQe7TYIXHd15dlDGT0omQ07C1i2Zm+wq9NpWdA3IaMz9c83\nbS88LIxbLh9LckIUS1bt4tPMQ8GuUqdkQd+EBJ/Px9bdhfSIi2zTyclN59IjNpJvf3k84WEe/vLC\np7y9PsfG6KnHgr4JCbn5Ryk+WsnYwcmW6+3mhqb34PvXTCQ22svTb+7gsf9so7KZAe1CiQV9ExKC\nPaqm6VijBiXzi/lTGdI3gfe3HGDBU+spOFwe7Gp1Chb0TUiou4g7phPelGXaR88e0fzXDZM5e0Jf\nsg6Wct//rTt+XSeUWdA33V5lVQ07sg/TPzWepPjQHIs+VEV4w5l/0WhunC0cq6zmT//eyNI1e0M6\nz29B33R7O3IOU1Vd2ynvwjUd45yJ/fjxDZNJio/i+Xcz+dtLWyivCM2buAKZGD0eeBJIBqKAe4F4\n4G6gEsgF5qtqpV+ZRJxZthKBUuB6VS0UkfOABUANsFRVf9W2h2PMF1lXTQMwLD2Rn8+fyj9e2sJ6\nzWdfwVHuuHI8fXvFBbtqHSqQlv58QFV1JnA18ADwIDBbVc/BCepX1itzF/Cuqs4AlgA/dpc/CFyF\nM9n6BSIyptVHYEwztu4uJMIbxsj+icGuigmyxLhIfnDtRC6YOoD9h8r41f+t45Md+cGuVocKJOgX\nAL3cx8nu80KgbvCSJHeZv1nAi+7jV4HzRGQoUKiq2e5k6Uvd7YxpN4dLK8jJP4oMSCLC23mHXjAd\nxxsexrWzRvCty8ZQW+vjoSWbWbIqk9ra0MjzN5veUdVFIjJfRDJwgv4cIBrYICKHgQ2q+la9Yn1w\nJj4HyAP61ltWt3xYU6+dnByLt5Uf1NTUhFaVN13bp3uKAJg2vm9QzgU7/zqvS89JYNyINBY8sZbX\nPtjLvsJy7r7hVBJiI4NdtXYVSE5/HpClqrNFZALwuFtuKrALWCwil6nqK43sorE7YZq9Q6aoqKy5\nTZpkE6ObNZ/uA2BQalyHnwt2/nV+8RFh/PSrp/LPVz7jk+15fPcP73DHleMZ2Ltrf1k31dgIJL0z\nHVgOoKqbgNGAR1UzVdUHvA1MqVdmH07LHqCf+9x/mf9yY9pFrc/H1j2FJMVH0i8ltC7WmcDFRUdw\n59WncOmZgykoPsaChev5cOuBYFer3QQS9DOAaQAiMgjIAZJFJNVdPxXYWa/MG8Bc9/FVwOuqugfo\nISKDRcQLXOJuZ0y7yD5YSklZFWOH9LQhik2TwsI8fPnsoXznqvGEh3t45NXP+P0zn/DvFRl8uOUA\n2Xml3WYe3kDGbn0YeExEVrrb3wIkAK+KSAWwG1gkIn2Ae1X1FpxeOk+JyGrgMDDP3ddtwLPu48Wq\nuqPtDsWYE9XdhWtdNU2gJo1I5Wc3TuWRV7eyPesw27MOH18XHuYhPSWOAWnx9E+NZ0DveAakxdOj\ni10D8HTmO9Py80taVTnLqYa2/312A9v2FnH/d2cE5YNp51/XVnasipz8o2TnlZKdV0J23lFy80up\nrD6xxZ8YF8mANOcLoL/7f5+esXjDg3fva2pqQqM/bW2WBtMtVVTWsDPnMIN6J3S5lpjpHGKjIxg5\nIOmEqTVra30cLCpzvwxKyD5YSk5+KVt2Fx4f1A/AG+7hzHF9mHeBBDX4N8SCvumWNLuI6hqfDb1g\n2lRYmIe+veLo2yuOqaPSji8/eqyKnLxS91dBKZp9mFWb9lN4pILbvzyuU01u33lqYkwbOj6Uso2q\naTpAXHQEMjAZGZgMQEVVDX9/aQufZh7if5/dyF1zT+k0/f871+8OY9rI1t2FREWEM6yfDb1gOl5U\nRDh3XDme6eP6sHv/EX7z1CccKj4W7GoBFvRNN3SwsIz9h8qQgUlEeO0UN8HhDQ/jG3NGM3vaQA4U\nlrHgqfXkFhwNdrUs6Jvu5/mVmQCcOa5PM1sa0748Hg/XzBzONTOHU1RSwW+fWk9GbnFQ62RB33Qr\nmlXEes1nWL8eJ1xoMyaYZk8byE1zRlNeUcMfnt3Ap5n1x6jsOBb0TbdR6/OxaEUGANfOGmF34ZpO\nZfr4vtxx1XgAHnx+Mx9s2R+UeljQN93Gh1sOsPdACaeP7c2wdLuAazqficNTuPvaSURHhvOv17ax\nfG1Wh9fBgr7pFo5VVvP8ykwivWFcfU6TI3YbE1TD+yfyX/MmkxQfyeIVGTz3TkaHztlrQd90C8vW\nZFFcWsmFpw2kZ4/oYFfHmCb1T43nJ189ld49Y1n2URaPL91OTW3HDOhmQd90eYVHjrF8bRaJ8ZFc\ndPrAYFfHmICkJMZwz7zJDOmbwHub9/PXJVuoqKpp99e1oG+6vOdXZlJZXcvV5wzrVLe7G9OcHrGR\n/PC6SYwdnMzGjAL+uHgjR49VtetrWtA3XVrmvmLWbD3IoD4JnGH98k0XFB3p5c65EzhtdBoZOcX8\n9ulPKCqpaLfXs6Bvuiyfz8eit535e66bNYIw66JpuihveBjfumwssyb3Jzf/KAsWrudAYeumi230\ntZrbQETigSdxJkWPAu4F7vHbJB14QlUX+JX5CzDefRoLHFbVC0SkCnjfr+wsVW3/JJbpltZuyyMz\n9winSuoJw98a0xWFeTxcf/4IesRF8OLq3bzwbibfvnJ88wVbKJAE6HxAVfUeEUkHVqjqqLqVIrIM\nWOhfQFW/47f+F8Bn7tNiVT23tZU2prKqhuffzcAb7mHuzOHBro4xbcLj8XDp9CEMTU8kMb59RuUM\nJL1TAPRyHye7zwEQkfOAHaqa3VBBEUkGZgHPt7Kexpxg+cfZHDpSwflTBpCWFBPs6hjTpsYO6Un/\n1Ph22XezQV9VFwEDRSQDWAUV2WynAAAaF0lEQVTc7bf6Tpz5cBvzTeBxVa278yBaRJ4RkfdF5Psn\nW2kT2g6XVrD0w70kxEZwyZmDg10dY7qUQHL684AsVZ0tIhOAR4EpItIPiFPVzCaKXw+c4ff8buAp\nwAesEpFVqrquscLJybF4veGBHEejUlMTWlXedD7PrsigoqqGmy4fx8D+ycGuTpPs/DOdTSA5/enA\ncgBV3SQi6SISDlwMrGiskIiMAApUtbxumar+w2/92zgXexsN+kVFrbt6bRNTdz97D5Tw1tos+qXG\nMWlocqf++9r5Z4KlqcZGIDn9DGAagIgMAkrdHjdTgU1NlDthvTieERGPiHhxvky2BvD6xgCfd9H0\nAdd+aQThYdbj2JiWCuRT8zAwWERWAs8At7rL+wJ5dRuJSB8Rediv3AnrVVWBbGAtTrfNpaq6tnXV\nN6Hkkx0FaPZhJgzrxdghNvetMSfD05Gju7VUfn5JqypnP6+7j6rqWn72r484dOQY9910Gn17xQW7\nSs2y888ES2pqQqN3KtrvY9MlvL0+h7zD5cyc1K9LBHxjOisL+qbTO1JWyasf7CYu2stlM4YEuzrG\ndGkW9E2n9/Lq3ZRX1HDZjCHEx0QEuzrGdGkW9E2nlptfyrsbc+nTM5aZk/oFuzrGdHkW9E2n5XMn\nOvf54JovDccbbqerMa1lnyLTaW3edYituwsZOziZCcN6NV/AGNMsC/qmU6quqWXxigw8HvjKrBF4\nbKx8Y9qEBX3TKa3cuI/9h8o4Z0J6u402aEwosqBvOp3MfcW8tHoXMVHhXHHW0GBXx5huxWaRNp1G\naXkVL6zMZNXGffiAeReMpEdc+0wkYUyosqBvgq7W5+P9T/fz3LuZlJZXkZ4Sx1cvGIkM7NzDJhvT\nFVnQN0GVdbCEp97YQUZuMVER4VwzczjnTelv3TONaScW9E1QlFdU89Lq3by9Podan48pksq1s0bQ\ns0d0sKtmTLdmQd90KJ/Px9pteSxasZPi0krSkmOYd/5Ixg21fvjGdAQL+qbD7D90lKfe2MG2vUV4\nw8O4YsYQLjp9IBGtnBLTGBO4QObIjQeeBJKBKOBe4B6/TdKBJ1R1gV+ZXwI3ALnuooWq+qiInAcs\nAGpwJlH5VVschOncKqpqeO2DPbz+URY1tT7GD+3FDeePIC05NthVMybkBNLSn48z8dU9IpIOrFDV\nUXUrRWQZsLCBcg+o6kP1lj0IXIjzZbBSRF5Q1c9OruqmK9i4s4Cn39zBoSPH6NkjiutmjWTyyBS7\nw9aYIAkk6BcAp7iPk93nALgt9x2qmt3cTkRkKFBYt62ILAVmARb0u6GCw+U889ZONmYUEB7m4aLT\nB3LZmUOIirRUjjHB1GzQV9VFIjJfRDJwgv4cv9V3Anc1UnSuiFwOVADfAfoA+X7r84BhJ1Vr0ylV\n19SyZVchH2w9wMad+VTX+JABScy7UOiXYrNdGdMZBJLTnwdkqepsEZkAPApMEZF+QJyqZjZQbClO\nGmiViFwL/AUnl++v2d/3ycmxeFt5kS81NaFV5U3TfD4fO7MP8876bFZtyOXI0UoABvSOZ+6skZw7\nuX9Ip3Ls/DOdTSDpnenAcgBV3SQi6SISDlwMrGiogKqu9Xv6CvA7YB9Oa79OP3dZo4qKygKoXuNs\nYur2U1BczpqtB/lgywEOFDp/p4TYCM6b0p8zx/VhUO8EPB4PBQWlQa5p8Nj5Z4KlqcZGIEE/A5gG\nvCAig4BSVa0RkanAqw0VEJEHgOdVdTVwLrBFVfeISA8RGQzkAJfg9PAxXUR5RTXrtufxwZYDaPZh\nALzhYZw2Oo0zxvZh7JCedietMZ1cIEH/YeAxEVnpbn+ru7wvTl4eABHpA9yrqrcA/wIeFpEqoBb4\nprvZbcCz7uPFqrqj9Ydg2lNNbS1bdxfywZYDbNhZQFV1LQAyIIkzxvVhiqQRG223exjTVXh8Pl+w\n69Co/PySVlXOfl6fvKyDJby/+QAffXaAI2VVAPTpGcsZ4/pwxpjepCTFBLmGnZ+dfyZYUlMTGr2Q\nZk008wXZeaX88vGPAYiPiWDW5P6cMa4PQ/omhPRFWWO6Awv65gsycpx8/WXTB3PJmYMtT29MN2Kf\nZvMFWXlOj5vJI1Mt4BvTzdgn2nxB1sESwsM8pNsNVcZ0Oxb0zQlqamvJyT9Kv5Q4a+Ub0w3Zp9qc\n4EBhOVXVtQzsbXeSGtMdWdA3J8g66HQxHNA7Psg1Mca0Bwv65gTZB52LuAPTLOgb0x1Z0Dcn2FvX\n0k+z9I4x3ZEFfXOcz+cjO6+U1KRoG1rBmG7Kgr45rqikgtLyKruIa0w3ZkHfHJdl+Xxjuj0L+ua4\nrDwnn28tfWO6Lwv65rjjLX0L+sZ0Wxb0zXFZB0uIj4kgKT4y2FUxxrQTC/oGgLJjVRQUH2NQ73gb\nPtmYbiyQidHjgSeBZCAKuBe4x2+TdOAJVV3gVybRLZOE88XyLVXdJiJ7gGygxt30BlXNbf1hmNbK\ndkfWHGCpHWO6tUA6Y88HVFXvEZF0YIWqjqpbKSLLgIX1ynwfeF9Vfy8ic3C+KK5x112kqqE7W3Yn\n9Xk+33ruGNOdBRL0C4BT3MfJ7nMAROQ8YIeqZtcr8xucuXEB8oFeraynaWd1Y+4MtDtxjenWmg36\nqrpIROaLSAZO0J/jt/pO4K4Gyhyrt80zfs//ISKDgfeAe1S10Xlwk5Nj8XrDm6tik1JTLYgFYl9h\nGZER4YyT3oSHWU6/rdj5ZzqbQHL684AsVZ0tIhOAR4EpItIPiFPVzCbK/g6oUNVH3UU/B14HCoGX\ngKuA5xsrX1RUFvCBNMQmpg5MdU0tWQdKGNQngcJDlnlrK3b+mWBpqrERSO+d6cByAFXdBKSLSDhw\nMbCisUIich+QBtxct0xVn1TVPFWtBpYC4wM5ANO+cvOPUlPrsztxjQkBgQT9DGAagIgMAkpVtQaY\nCmxqqICIzABOA25S1Vp3WaKILBeRuk7g5wBbWll/0wbsTlxjQkcgQf9hYLCIrMTJzd/qLu8L5NVt\nJCJ9RORh9+ntwEBghYi8KyJLVLUYp3W/RkTex7nA22hqx3Scup47NnGKMd2fx+dr9Dpq0OXnl7Sq\ncpZTDcxvn1rPztxi/vb9c4iKaN2Fc/M5O/9MsKSmJjTaG8PuyA1xtT4fWXml9OkZawHfmBBgQT/E\nFRwu51hljeXzjQkRFvRDnN2Ja0xosaAf4o733LE7cY0JCRb0Q5z13DEmtFjQD3FZB0tIToiiR6yN\noW9MKLCgH8KOHK3kcGklA+xOXGNChgX9EFY3hr713DEmdFjQD2GfD6dsLX1jQoUF/RCWlWfdNY0J\nNRb0Q1jWwRJiosJJSYoJdlWMMR3Egn6Iqqis4cChMgakxhNmE6EbEzIs6IeonIJSfNhFXGNCjQX9\nEGU3ZRkTmizoh6hsmwjdmJAUyBy58cCTOJOiRwH3Avf4bZIOPKGqC/zKJOJMuJIIlALXq2qhiJwH\nLABqgKWq+qu2OhDTMnsPlhIe5qFfalywq2KM6UCBtPTnA6qqM4GrgQdU9dy6f0AmsLBembuAd1V1\nBrAE+LG7/EGcydCnAxeIyJjWH4JpqZraWnLyS0lPicMbbj/2jAklgXziC4Be7uNk9zkAbst9h6pm\n1yszC3jRffwqcJ6IDAUKVTXbnTd3qbud6WAHC8upqq61/vnGhKBmg76qLgIGikgGsAq422/1nTit\n9/r64MyBC848un3rLfNfbjpYluXzjQlZgeT05wFZqjpbRCYAjwJTRKQfEKeqmc3sorFO4M12Dk9O\njsXrbd0UfqmpFtjqKyjJAmC8pNn7087s/TWdTbNBHyf/vhxAVTeJSLqIhAMXAysaKbMPp2VfDPRz\nn9ctq1O3vFFFRWUBVK9xNjF1w7bvOQRAj8hwe3/akZ1/JliaamwEktPPAKYBiMggoFRVa4CpwKZG\nyrwBzHUfXwW8rqp7gB4iMlhEvMAl7namA/l8PrIOlpKSGE1sdCDf+caY7iSQoP8wMFhEVuJ0w7zV\nXd4XJy8PgIj0EZGH3acP4qSAVgMzgf91l98GPAusBhar6o7WH4JpicOllZSWVzHI7sQ1JiR5fD5f\nsOvQqPz8klZVzn5ef9HGjAIefP5TrjhrCJdNHxLs6nRrdv6ZYElNTWj0mql10g4xdieuMaHNgn6I\nqRtzx/roGxOaLOiHmKy8EuJjIkhOiAp2VYwxQWBBP4SUHasm//AxBvaOx2Nj6BsTkizoh5DsPMvn\nGxPqLOiHEJsT1xhjQT+E1I25M8D66BsTsizoh5Dsg6VEeMPo09MmQjcmVFnQDxHVNbXkFhylf2o8\n4WH2ZzcmVNmnP0TsKzhKTa3P8vnGhDgL+iHi85uyLJ9vTCizoN+AvQdKePD5T49f+OwOPp84xVr6\nxoQyG1u3nsx9xfxp8SbKK6qpqq7hB9dOCnaV2kRWXikeoH+qBX1jQpm19P3syD7MHxdtpKKyhpTE\naLbuKeoWrf1an4/svBL69IolKrJ1M5EZY7o2C/qu7XuL+PO/N1FVXcutl49l3gUjAXh9bVaQa9Z6\nBcXHKK+oYYCldowJeRb0ga27C7n/uU1U19Ry+xXjmDIqjfFDe9EvJY61n+VxqPhYsKvYKnXDKdvE\nKcaYQCZGjweeBJKBKOBeYA2wCOgJ5ALXqWqFX5mfAue7T8OAPqo6UkT2ANlAjbvuBlXNbZMjOUmb\nMgr464tbAPjOVadwyrBeAHg8Hi48bSCPLd3Gm+uyuXbWiGBWs1X2uj13Blh3TWNCXiAt/fmAqupM\n4GrgAeCnwBuqOg3YCEzwL6Cqv1bVc1X1XOBR4BG/1RfVrQt2wF+v+Ty0ZDNhHrhz7ucBv87pY3uT\nFB/Jyk37KDtWFaRatp5NnGKMqRNI0C8A6qJhsvv8UuBpAFW9T1XXNlTQnQD9NuCh1le1ba3ddpC/\nv7QFb3gY37tmAmMH9/zCNt7wMM6fMoCKyhre2RDU76dWycorJSk+kh5xkcGuijEmyJpN76jqIhGZ\nLyIZOEF/DrAMuFVEzgc+A77rn97xcyWwXFXL/Zb9Q0QGA+8B96hqo/PgJifH4vW2rrdJauoXW7fv\nrM/mn69sJTrKyy9vPoPRQ74Y8OtcdZ7w2od7WfFJLjdcPIaIVtanoxWXVlBUUsGU0b0bfC9M+7L3\n3HQ2geT05wFZqjpbRCbgpGuigTdV9T4ReQS4GfhrA8VvAm7xe/5z4HWgEHgJuAp4vrHXLioqC/Q4\nGtTQxNSrN+3jiWXbiYny8oOvTCQlPqLZyavPntCX5WuzeXVlBmedkt6qOnW0rXsKAeiTHG2TdHcw\nmxjdBEtTjY1A0jvTgeUAqroJSAdyVPVDd/0bwNj6hUQkDuivqnvqlqnqk6qap6rVwFJgfIDH0Cbe\n2ZDL48u2ExcTwQ+vm8SQvj0CKnf+lAGEh3lYvjabWl+jP0zaVHVNbZvsJ8vy+cYYP4EE/QxgGoCI\nDAJKgbdFZKa7/lRAGyg3Adhe90REEkVkuYjUJZbPAbacbMVb6s2Ps1m4XOkRG8GPrpvEoD6BB8Ge\nPaI5bXRv9hUcZXPmoXaspWPrnkLu+PMqnli2rdXBP9t67hhj/AQS9B8GBovISuAZ4FbgZ8A9IrIa\nGA78C0BEXvYr1xfIq3uiqsU4rfs1IvI+kE8TqZ22tGzNXp59eyeJ8ZH8+IbJ9D+Jm5RmTxsIwOsf\nte/NWuUV1TyxdBuV1bWs2rSfPy3eyNFW9Bzae7CE6MhwUpNsDH1jDHh8HZSuOBn5+SWtqlxqagKP\nvvQpL63eTXJCFD+6bhK9e8ae9P7+tHgjW3YX8rMbpwScGmqphcuVdzbkcsHUARQUH+OTHfn06RnL\nnXNPoXdyy+peUVXD7X9ayfB+idwz79R2qa9pnOX0TbCkpiZ4GlvXbe/I9fl8PLVsGy+t3k1KYjT/\ndcPkVgV8gAvd1v6ydmrtb99bxDsbcumXEsdV5wzj9i+PY/a0gRwoLOPXT65nR/bhFu0vN/8oPp8N\np2yM+Vy3DPo+n4/n3s1k8Vs7SEuK4cfXT26T9MaYQckMTItnveaRd7i8+QItUFFZw+PLtuHxwNcv\nHk2EN4wwj4drZg7nxtlCeUU1f1i0gQ+3HAh4nzacsjGmvm4Z9DP3HeH1j7LonxbPj2+YTK/E6DbZ\nr8fjYfa0gfh88EYbD8S2ZNUu8g8f48LTBjI0/cTU0TkT+/G9ayYQ4Q3nkdc+48VVuwgkLZeVZxOn\nGGNO1C2D/oC0eOZdMJLf3D6D5ISoNt33lFFp9OoRxXuf7qekrLJN9rkz5zBvrcumd89YrpgxpMFt\nxgzuyU+/eiqpSdG8+sEeHn5lK1XVNQ1uWyfrYAnhYR7SU+LapJ7GmK6vWwb9qIhwvjS5P0ltHPDB\nHZph6kAqq2t555PWD81QWVXDY0udnq3fuHgUkRGN3/GbnhLHT782heH9E1m7LY/fP7uBI0cb/uKp\nrfWRk1dK315xRHi75Z/ZGHMSLBqchLMn9CU2ysvbn+RQWdV0a7s5L7+3m4OFZcya0p8R/ZOa3b5H\nbCQ/vHYip4/pTWbuEf7nyXXkFhz9wnYHi8qorK5lkPXPN8b4saB/EqIjvcyc3I+Ssireb8GF1fp2\n7z/C62uzSE2K5qqzhwVcLsIbzjcvHcPlM4ZQUHyMBQvXsXV34Qnb7HUv4g6wfL4xxo8F/ZM069T+\neMM9LF+bRW1ty28nqKqu5bH/bMPng/kXjW7xNIYej4fLZwzhm5eOoaq6lj//exPv+o0EWncnrvXc\nMcb4s6B/kpLiozhjbB/yisrZsDO/xeVf+2APuQVHOXdSP0YPSj7pepwxtg8/vG4SsdFenlyuLHp7\nJ7W1vs+7a1p6xxjjx4J+K1x42udDM7TkzuasgyUsXbOXXj2imHtu4Gmdxozon8R/3ziFvr1ieePj\nbB5aspm9B0tJSYwmNjqi1fs3xnQfFvRbIT0ljonDU8jcd4SdOcUBlamucdI6NbU+bpw9ipioZke3\nDkhaUgw//eqpjB6UzMaMAkrLq6x/vjHmCyzot1JLB2JbtmYvWXmlzBjfl3FDezVfoAVioyP43jUT\nOHtCXwCG9Wuf8YGMMV1X2zQzQ9iI/okMTe/BxowC9h86St9ejd8IlZtfyivv7yExPpJrZw1vl/p4\nw8O4cfYovjS5v92UZYz5Amvpt5LH42G2m9tf3sTQDDW1tTy21E3rXDiqXXPtHo+Hgb0T8Ibbn9cY\ncyKLCm1g8shU0pJj+GDLAYpLG5oqGN74OJvd+0s4fWxvJo5I6eAaGmOMI5A5cuOBJ3EmRY8C7gXW\nAIuAnkAucJ3/xOgiMh/4FZDpLnpTVX/tzrH7d8AHfKqqt7XdoQRPWJiHC6cOYOEbO3hrfQ5XnXNi\nj5z9h47y4qrd9IiN4PrzRgaplsYYE1hLfz6gqjoTuBp4APgp8IaqTgM24kyNWN9iVT3X/fdrd9n9\nwJ2qOh1IFJGLWn0EncT08X2Jj4ng3Q25HKusPr68ttbH40u3U11Ty7wLhPgY60JpjAmeQIJ+AVDX\nzSTZfX4p8DSAqt6nqmub24k7N+4QVf3YXfQqcF6La9xJRUaEM+vU/hw9Vs3qTfuPL397fQ4ZucVM\nkVSmjEoLYg2NMSaA9I6qLhKR+SKSgRP05wDLgFtF5HzgM+C7/ukd1zki8joQAdwNHASK/Nbn4cyj\n26jk5Fi83pYNT1BfamrH9VWfe76w7KMs3v4kh69cOIq8onJeWLWLhNhIvnvdZJIT2mZcf9N1dOT5\nZ0wgAsnpzwOyVHW2m5N/FIjGydPfJyKPADcDf/UrtgbIV9X/iMgZONcELqy360bncKxTVFQW4GE0\nLBhzlE4f34d3Psll6epMVm3aR2VVDfNnC9XHqshvxQTnpuuxOXJNsDTV2AgkvTMdWA6gqpuAdCBH\nVT90178BjPUvoKrbVfU/7uMPgVTgEJ+niQD6AfsCO4Su48KpA/B44P+WK9uzDjNxeArTxvQOdrWM\nMQYILOhnANMARGQQUAq8LSIz3fWnAupfQER+JCLXuY/H4bT6K4DtIjLD3exK4PXWH0LnkpYcy6kj\nU6morCE2ystXLxQ8nmZ/1BhjTIcIJOg/DAwWkZXAM8CtwM+Ae0RkNTAc+BeAiLzslnkG+JZb5mHg\nJnf5XcBvROR9IFNV32qzI+lELjlzMInxkXxttrT5dI3GGNManpaMDtnR8vNLWlU5y6maYLLzzwRL\nampCo+kFuyPXGGNCiAV9Y4wJIRb0jTEmhFjQN8aYEGJB3xhjQogFfWOMCSEW9I0xJoRY0DfGmBDS\nqW/OMsYY07aspW+MMSHEgr4xxoQQC/rGGBNCLOgbY0wIsaBvjDEhxIK+McaEEAv6xhgTQizoG2NM\nCPEGuwLBICJ9gA3AAFWtDnZ9TOgQkek4U45GAv+rquuCXCUTYrps0HcnXH8Z+LOqPuQu+zNwOuAD\n7lTVjxsp/n1gZYdU1HRLrTj/jgDfBE4BzgUs6JsO1SWDvojEAX8B3vZbdg4wQlXPEJHRwGPAGSJy\nFzDD3WwrsBNYgtPaMqbFWnP+qeovRORi4G6c4G9Mh+qSQR+oAC4Gfuy3bBbwEoCqbhORZBHpoar3\nA/fXbSQiDwHDgYnAtcBTHVZr01205vybBiwD1gK/BO7oqEobA1006Lt5+GoR8V/cB1jv9zzfXXak\nXtk7AERkMLCoXStquqXWnH9AMvAwEIc1OEwQdMmgHyBPUytVdX4H1cOEpgbPP1V9HXi9g+tizHHd\nqcvmPpyWVZ10YH+Q6mJCj51/pkvoTkH/DeBqABGZDOxT1ZLgVsmEEDv/TJfQJSdREZFTgT8Cg4Eq\nIBe4EvgRcDZQC3xbVTcFq46m+7Lzz3RlXTLoG2OMOTndKb1jjDGmGRb0jTEmhFjQN8aYEGJB3xhj\nQogFfWOMCSEW9I0xJoRY0DfGmBBiQd8YY0KIBX1jjAkh/w+Ut7S1p0TtDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "L21VsgB-QocE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
      ]
    },
    {
      "metadata": {
        "id": "Y2fEp5TFQocF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
        "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "lbWrUcB5QocI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0149cf29-2b1a-40a2-93d7-245b0f7bfbc1"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 617.500061\n",
            "Minibatch accuracy: 6.2%\n",
            "Validation accuracy: 29.5%\n",
            "Minibatch loss at step 500: 194.213608\n",
            "Minibatch accuracy: 82.0%\n",
            "Validation accuracy: 80.0%\n",
            "Minibatch loss at step 1000: 115.878586\n",
            "Minibatch accuracy: 79.7%\n",
            "Validation accuracy: 81.5%\n",
            "Minibatch loss at step 1500: 68.543327\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 83.0%\n",
            "Minibatch loss at step 2000: 41.113152\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 85.4%\n",
            "Minibatch loss at step 2500: 25.226456\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 86.6%\n",
            "Minibatch loss at step 3000: 15.529140\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 87.2%\n",
            "Test accuracy: 92.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6GNCPPCuQocN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "na8xX2yHZzNF"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "vE7s_ccUeExm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$ layer_1 = ReLU(w_1 X + b_1)$   \n",
        "\n",
        "$ logits = layer_1w_2 + b_2$     \n",
        "\n",
        "$ loss = softmax(logits, y)$  \n",
        "$ optimizer = GradientDescent(loss) $"
      ]
    },
    {
      "metadata": {
        "id": "6B6IZ57zQoca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFbIQ1nfQocd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2652
        },
        "outputId": "dfa51544-6b05-4711-dc4e-3997a9da21be"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 101\n",
        "num_batches = 3\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 2 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 289.295166\n",
            "Minibatch accuracy: 10.9%\n",
            "Validation accuracy: 16.1%\n",
            "Minibatch loss at step 2: 2396.713379\n",
            "Minibatch accuracy: 27.3%\n",
            "Validation accuracy: 42.9%\n",
            "Minibatch loss at step 4: 752.467896\n",
            "Minibatch accuracy: 32.8%\n",
            "Validation accuracy: 52.8%\n",
            "Minibatch loss at step 6: 246.957642\n",
            "Minibatch accuracy: 64.1%\n",
            "Validation accuracy: 66.6%\n",
            "Minibatch loss at step 8: 75.664871\n",
            "Minibatch accuracy: 79.7%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 10: 30.007084\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 12: 46.787003\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 70.4%\n",
            "Minibatch loss at step 14: 22.872910\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 16: 2.435671\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 71.0%\n",
            "Minibatch loss at step 18: 4.935873\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 20: 0.326478\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 72.8%\n",
            "Minibatch loss at step 22: 0.291470\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 72.8%\n",
            "Minibatch loss at step 24: 0.000451\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 26: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 28: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 30: 0.000003\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 32: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 34: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 36: 0.000003\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 38: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 40: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 42: 0.000003\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 44: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 46: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 48: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 50: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 52: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 54: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 56: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 58: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 60: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 62: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 64: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 66: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 68: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 70: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 72: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 74: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 76: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 78: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 80: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 82: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 84: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 86: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 88: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 90: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 92: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 94: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 96: 0.000002\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 98: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Minibatch loss at step 100: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.9%\n",
            "Test accuracy: 80.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9f5MKb0fQoch",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ww3SCBUdlkRc"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
        "\n",
        "What happens to our extreme overfitting case?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "gfyy_hccf0nw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dropout() Technical to improve the Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "c5ft7SJbQocj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
        "  logits = tf.matmul(drop1, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVqrzK3lQocl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2652
        },
        "outputId": "18e6fb21-739a-44d7-c169-eab26dd0df67"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 101\n",
        "num_batches = 3\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    offset = step % num_batches\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 2 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 445.536011\n",
            "Minibatch accuracy: 13.3%\n",
            "Validation accuracy: 25.1%\n",
            "Minibatch loss at step 2: 1185.745117\n",
            "Minibatch accuracy: 50.0%\n",
            "Validation accuracy: 44.0%\n",
            "Minibatch loss at step 4: 240.813293\n",
            "Minibatch accuracy: 67.2%\n",
            "Validation accuracy: 64.8%\n",
            "Minibatch loss at step 6: 3.243047\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 8: 4.985995\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 10: 2.050717\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 70.3%\n",
            "Minibatch loss at step 12: 3.248143\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 70.5%\n",
            "Minibatch loss at step 14: 1.600573\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 69.4%\n",
            "Minibatch loss at step 16: 1.602779\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 69.2%\n",
            "Minibatch loss at step 18: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.2%\n",
            "Minibatch loss at step 20: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.2%\n",
            "Minibatch loss at step 22: 0.031912\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.0%\n",
            "Minibatch loss at step 24: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.9%\n",
            "Minibatch loss at step 26: 0.224731\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 28: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 30: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 32: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 34: 1.479093\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 70.2%\n",
            "Minibatch loss at step 36: 0.390927\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.3%\n",
            "Minibatch loss at step 38: 0.166471\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.5%\n",
            "Minibatch loss at step 40: 0.191273\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.5%\n",
            "Minibatch loss at step 42: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.5%\n",
            "Minibatch loss at step 44: 1.859560\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.5%\n",
            "Minibatch loss at step 46: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.5%\n",
            "Minibatch loss at step 48: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.5%\n",
            "Minibatch loss at step 50: 0.964147\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 52: 0.524230\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 71.6%\n",
            "Minibatch loss at step 54: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 71.6%\n",
            "Minibatch loss at step 56: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 71.6%\n",
            "Minibatch loss at step 58: 0.073121\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 71.4%\n",
            "Minibatch loss at step 60: 0.081333\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 71.8%\n",
            "Minibatch loss at step 62: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 71.8%\n",
            "Minibatch loss at step 64: 0.854001\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 66: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 68: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 70: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 72: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 74: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 76: 0.000514\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 78: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 80: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 82: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 84: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.3%\n",
            "Minibatch loss at step 86: 0.000405\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.3%\n",
            "Minibatch loss at step 88: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.4%\n",
            "Minibatch loss at step 90: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.4%\n",
            "Minibatch loss at step 92: 1.194516\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 94: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 96: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 72.5%\n",
            "Minibatch loss at step 98: 0.314102\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 72.4%\n",
            "Minibatch loss at step 100: 1.152177\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 72.5%\n",
            "Test accuracy: 79.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mJbiPnt1Qocq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-b1hTz3VWZjw"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 4\n",
        "---------\n",
        "\n",
        "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
        "\n",
        "One avenue you can explore is to add multiple layers.\n",
        "\n",
        "Another one is to use learning rate decay:\n",
        "\n",
        "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        " \n",
        " ---\n"
      ]
    },
    {
      "metadata": {
        "id": "mKIlvW4ZQocu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
      ]
    },
    {
      "metadata": {
        "id": "PNzGVxU9Qocv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes1 = 1024\n",
        "num_hidden_nodes2 = 100\n",
        "beta_regul = 1e-3\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  global_step = tf.Variable(0)\n",
        "\n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "        [image_size * image_size, num_hidden_nodes1],\n",
        "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
        "    )\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
        "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
        "  weights3 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
        "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
        "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
        "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
        "  \n",
        "  # Optimizer.\n",
        "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCTLjPTHQoc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "b4d03480-3ba3-4e6b-8091-a68e42baa1fd"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 9001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3.282810\n",
            "Minibatch accuracy: 8.6%\n",
            "Validation accuracy: 31.1%\n",
            "Minibatch loss at step 500: 0.883438\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 85.4%\n",
            "Minibatch loss at step 1000: 0.877508\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 86.9%\n",
            "Minibatch loss at step 1500: 0.728249\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 88.3%\n",
            "Minibatch loss at step 2000: 0.488919\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 88.8%\n",
            "Minibatch loss at step 2500: 0.581660\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 3000: 0.517112\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 3500: 0.473228\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.2%\n",
            "Minibatch loss at step 4000: 0.502941\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.4%\n",
            "Minibatch loss at step 4500: 0.417903\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 89.7%\n",
            "Minibatch loss at step 5000: 0.585046\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 89.9%\n",
            "Minibatch loss at step 5500: 0.474939\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 6000: 0.405357\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 6500: 0.478851\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 7000: 0.495478\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 7500: 0.545219\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 8000: 0.396618\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 8500: 0.426969\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 9000: 0.353534\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.5%\n",
            "Test accuracy: 95.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZy1QbB2Qoc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is getting really good. Let's try one layer deeper with dropouts."
      ]
    },
    {
      "metadata": {
        "id": "OelbNCwgQoc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes1 = 1024\n",
        "num_hidden_nodes2 = 256\n",
        "num_hidden_nodes3 = 128\n",
        "keep_prob = 0.5\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  global_step = tf.Variable(0)\n",
        "\n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "        [image_size * image_size, num_hidden_nodes1],\n",
        "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
        "    )\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
        "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
        "  weights3 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
        "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
        "  weights4 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
        "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
        "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
        "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "  \n",
        "  # Optimizer.\n",
        "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
        "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hEA5aH1Qoc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1938
        },
        "outputId": "ded91cb0-53ff-4b98-f12c-ea3e5a969057"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 18001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 2.429001\n",
            "Minibatch accuracy: 6.2%\n",
            "Validation accuracy: 30.2%\n",
            "Minibatch loss at step 500: 0.304245\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 85.9%\n",
            "Minibatch loss at step 1000: 0.469985\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 86.8%\n",
            "Minibatch loss at step 1500: 0.413725\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 88.5%\n",
            "Minibatch loss at step 2000: 0.209225\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 88.7%\n",
            "Minibatch loss at step 2500: 0.381472\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 89.2%\n",
            "Minibatch loss at step 3000: 0.249260\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 89.5%\n",
            "Minibatch loss at step 3500: 0.310160\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 89.2%\n",
            "Minibatch loss at step 4000: 0.233052\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 89.2%\n",
            "Minibatch loss at step 4500: 0.196716\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 5000: 0.366630\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 90.0%\n",
            "Minibatch loss at step 5500: 0.165687\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 6000: 0.215245\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 6500: 0.220413\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 90.0%\n",
            "Minibatch loss at step 7000: 0.272434\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 7500: 0.223279\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 8000: 0.163696\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 8500: 0.130215\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.7%\n",
            "Minibatch loss at step 9000: 0.114361\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.7%\n",
            "Minibatch loss at step 9500: 0.120837\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 10000: 0.020323\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 10500: 0.157257\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.6%\n",
            "Minibatch loss at step 11000: 0.120834\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.7%\n",
            "Minibatch loss at step 11500: 0.103505\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 12000: 0.122927\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 12500: 0.154228\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 13000: 0.040849\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 13500: 0.052288\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 14000: 0.055827\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 14500: 0.086491\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 15000: 0.060589\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 15500: 0.143034\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 16000: 0.049673\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 16500: 0.025750\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 17000: 0.038187\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 17500: 0.052880\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.4%\n",
            "Minibatch loss at step 18000: 0.025621\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.2%\n",
            "Test accuracy: 96.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQQXCipQQodA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
      ]
    }
  ]
}