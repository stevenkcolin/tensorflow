{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenkcolin/tensorflow/blob/master/3_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kR-4eNdK6lYS"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 3\n",
        "------------\n",
        "\n",
        "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
        "\n",
        "The goal of this assignment is to explore regularization techniques."
      ]
    },
    {
      "metadata": {
        "id": "oE-5yumhQw-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d533f589-94dc-408b-b21f-ee86643db22e"
      },
      "cell_type": "code",
      "source": [
        "!ls -lt"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 924428\n",
            "-rw-r--r--  1 root   root 690800441 Jan 28 16:06 notMNIST.pickle\n",
            "drwxrwxr-x 12 133040 5000      4096 Jan 28 15:53 notMNIST_small\n",
            "drwxrwxr-x 12 133040 5000      4096 Jan 28 15:53 notMNIST_large\n",
            "-rw-r--r--  1 root   root   8458043 Jan 28 15:46 notMNIST_small.tar.gz\n",
            "-rw-r--r--  1 root   root 247336696 Jan 28 15:46 notMNIST_large.tar.gz\n",
            "drwxr-xr-x  1 root   root      4096 Jan  8 17:15 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "JLpLa8Jt7Vu4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EmdlUCe-Qobd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some personnal imports\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1HrCK6e17WzV"
      },
      "cell_type": "markdown",
      "source": [
        "First reload the data we generated in _notmnist.ipynb_."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "y3-cj1bpmuxc",
        "outputId": "e80506b9-3476-4604-d288-74fde3b29590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset = save['train_dataset']\n",
        "  train_labels = save['train_labels']\n",
        "  valid_dataset = save['valid_dataset']\n",
        "  valid_labels = save['valid_labels']\n",
        "  test_dataset = save['test_dataset']\n",
        "  test_labels = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset.shape, train_labels.shape)\n",
        "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "  print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L7aHrm6nGDMB"
      },
      "cell_type": "markdown",
      "source": [
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "- data as a flat matrix,\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "IRSyYiIIGIzS",
        "outputId": "ca20f0ee-a819-4f1a-dcf2-82572cfece5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
        "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "RajPLaL_ZW6w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sgLbUAQ1CW-1"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "H1z-zyuCQobx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start with the logistic model:"
      ]
    },
    {
      "metadata": {
        "id": "N1GSfYVNQoby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rk_cHRNkQob1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e30abfe1-e1c0-4661-c1b7-1790dab3a566"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 22.291626\n",
            "Minibatch accuracy: 5.5%\n",
            "Validation accuracy: 7.8%\n",
            "Minibatch loss at step 500: 2.733181\n",
            "Minibatch accuracy: 71.9%\n",
            "Validation accuracy: 75.8%\n",
            "Minibatch loss at step 1000: 1.581475\n",
            "Minibatch accuracy: 78.1%\n",
            "Validation accuracy: 78.9%\n",
            "Minibatch loss at step 1500: 1.477900\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 80.3%\n",
            "Minibatch loss at step 2000: 0.782139\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 81.8%\n",
            "Minibatch loss at step 2500: 0.790150\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 81.4%\n",
            "Minibatch loss at step 3000: 0.860601\n",
            "Minibatch accuracy: 74.2%\n",
            "Validation accuracy: 81.6%\n",
            "Test accuracy: 88.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U_1I9RpZQob6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
      ]
    },
    {
      "metadata": {
        "id": "DLi5Ms6fQob7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
        "accuracy_val = []\n",
        "\n",
        "for regul in regul_val:\n",
        "  with tf.Session(graph=graph) as session:\n",
        "    tf.initialize_all_variables().run()\n",
        "    for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
        "      _, l, predictions = session.run(\n",
        "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "icN8Oc7iQob_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "0b0bcda2-5d85-4d1e-8e78-f48112a045cb"
      },
      "cell_type": "code",
      "source": [
        "plt.semilogx(regul_val, accuracy_val)\n",
        "plt.grid(True)\n",
        "plt.title('Test accuracy by regularization (logistic)')\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAELCAYAAAAiIMZEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJO7UkrFIE3uSikVgzS5K6VUDPr/qNh4eIg0wBkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "L21VsgB-QocE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
      ]
    },
    {
      "metadata": {
        "id": "Y2fEp5TFQocF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
        "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "lbWrUcB5QocI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4b5ba03c-4387-47d1-8324-3a30fc70142a"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 644.781006\n",
            "Minibatch accuracy: 5.5%\n",
            "Validation accuracy: 31.6%\n",
            "Minibatch loss at step 500: 194.346420\n",
            "Minibatch accuracy: 83.6%\n",
            "Validation accuracy: 79.7%\n",
            "Minibatch loss at step 1000: 115.568283\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 81.3%\n",
            "Minibatch loss at step 1500: 69.417992\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 83.5%\n",
            "Minibatch loss at step 2000: 41.141335\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 85.3%\n",
            "Minibatch loss at step 2500: 25.216549\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 86.4%\n",
            "Minibatch loss at step 3000: 15.503594\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 87.0%\n",
            "Test accuracy: 92.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6GNCPPCuQocN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "na8xX2yHZzNF"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "6B6IZ57zQoca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  beta_regul = tf.placeholder(tf.float32)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFbIQ1nfQocd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2652
        },
        "outputId": "73683fb1-40fd-4350-9ccc-83edad778cd3"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 101\n",
        "num_batches = 3\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 2 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 306.391846\n",
            "Minibatch accuracy: 4.7%\n",
            "Validation accuracy: 34.1%\n",
            "Minibatch loss at step 2: 2264.098145\n",
            "Minibatch accuracy: 18.0%\n",
            "Validation accuracy: 29.7%\n",
            "Minibatch loss at step 4: 582.575073\n",
            "Minibatch accuracy: 41.4%\n",
            "Validation accuracy: 63.1%\n",
            "Minibatch loss at step 6: 75.015572\n",
            "Minibatch accuracy: 83.6%\n",
            "Validation accuracy: 72.4%\n",
            "Minibatch loss at step 8: 28.861536\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 74.2%\n",
            "Minibatch loss at step 10: 11.188910\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 73.9%\n",
            "Minibatch loss at step 12: 0.918898\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 74.0%\n",
            "Minibatch loss at step 14: 1.791705\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 16: 0.572021\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 18: 0.137748\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 20: 0.276374\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 22: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 24: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 26: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 28: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 30: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 32: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 34: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 36: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 38: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 40: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 42: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 44: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 46: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 48: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 50: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 52: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 54: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 56: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 58: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 60: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 62: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 64: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 66: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 68: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 70: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 72: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 74: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 76: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 78: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 80: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 82: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 84: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 86: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 88: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 90: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 92: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 94: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 96: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 98: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Minibatch loss at step 100: 0.000001\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 74.3%\n",
            "Test accuracy: 81.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9f5MKb0fQoch",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ww3SCBUdlkRc"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
        "\n",
        "What happens to our extreme overfitting case?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "c5ft7SJbQocj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
        "  logits = tf.matmul(drop1, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "    \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVqrzK3lQocl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2652
        },
        "outputId": "1c8ff8cb-3d08-4379-f33e-6a10b856cb59"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 101\n",
        "num_batches = 3\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    offset = step % num_batches\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 2 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 499.107605\n",
            "Minibatch accuracy: 7.0%\n",
            "Validation accuracy: 27.8%\n",
            "Minibatch loss at step 2: 1398.270752\n",
            "Minibatch accuracy: 35.9%\n",
            "Validation accuracy: 42.7%\n",
            "Minibatch loss at step 4: 233.358551\n",
            "Minibatch accuracy: 70.3%\n",
            "Validation accuracy: 60.6%\n",
            "Minibatch loss at step 6: 28.538612\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 65.1%\n",
            "Minibatch loss at step 8: 3.151541\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 68.9%\n",
            "Minibatch loss at step 10: 0.003564\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 12: 0.944493\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 14: 1.356522\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 68.8%\n",
            "Minibatch loss at step 16: 3.243587\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 18: 2.621352\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.4%\n",
            "Minibatch loss at step 20: 0.016477\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.5%\n",
            "Minibatch loss at step 22: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.4%\n",
            "Minibatch loss at step 24: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.2%\n",
            "Minibatch loss at step 26: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.2%\n",
            "Minibatch loss at step 28: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.6%\n",
            "Minibatch loss at step 30: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.6%\n",
            "Minibatch loss at step 32: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.7%\n",
            "Minibatch loss at step 34: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 36: 0.452733\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 38: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.8%\n",
            "Minibatch loss at step 40: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 42: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 44: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 46: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 48: 0.000194\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.9%\n",
            "Minibatch loss at step 50: 1.841760\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.7%\n",
            "Minibatch loss at step 52: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.7%\n",
            "Minibatch loss at step 54: 2.313510\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.1%\n",
            "Minibatch loss at step 56: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 58: 0.011053\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 60: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.9%\n",
            "Minibatch loss at step 62: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.9%\n",
            "Minibatch loss at step 64: 0.078947\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 66: 0.275997\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 68: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 70.0%\n",
            "Minibatch loss at step 70: 0.534096\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 72: 0.283646\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 69.1%\n",
            "Minibatch loss at step 74: 1.142546\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.1%\n",
            "Minibatch loss at step 76: 4.130272\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 78: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 80: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 82: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 84: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 86: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.7%\n",
            "Minibatch loss at step 88: 0.376684\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 90: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 92: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 94: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 96: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 98: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Minibatch loss at step 100: 0.000000\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 69.8%\n",
            "Test accuracy: 76.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mJbiPnt1Qocq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-b1hTz3VWZjw"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 4\n",
        "---------\n",
        "\n",
        "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
        "\n",
        "One avenue you can explore is to add multiple layers.\n",
        "\n",
        "Another one is to use learning rate decay:\n",
        "\n",
        "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        " \n",
        " ---\n"
      ]
    },
    {
      "metadata": {
        "id": "mKIlvW4ZQocu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
      ]
    },
    {
      "metadata": {
        "id": "PNzGVxU9Qocv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes1 = 1024\n",
        "num_hidden_nodes2 = 100\n",
        "beta_regul = 1e-3\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  global_step = tf.Variable(0)\n",
        "\n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "        [image_size * image_size, num_hidden_nodes1],\n",
        "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
        "    )\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
        "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
        "  weights3 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
        "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
        "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
        "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
        "  \n",
        "  # Optimizer.\n",
        "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCTLjPTHQoc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "83b58572-f8e1-4647-c5c9-c70947572e07"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 9001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3.251631\n",
            "Minibatch accuracy: 2.3%\n",
            "Validation accuracy: 26.2%\n",
            "Minibatch loss at step 500: 0.876136\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 85.3%\n",
            "Minibatch loss at step 1000: 0.870243\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 86.7%\n",
            "Minibatch loss at step 1500: 0.710446\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 88.2%\n",
            "Minibatch loss at step 2000: 0.475630\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 88.4%\n",
            "Minibatch loss at step 2500: 0.606912\n",
            "Minibatch accuracy: 85.2%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 3000: 0.525435\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 3500: 0.458693\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.3%\n",
            "Minibatch loss at step 4000: 0.504208\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 89.5%\n",
            "Minibatch loss at step 4500: 0.413816\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 89.8%\n",
            "Minibatch loss at step 5000: 0.593164\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 89.8%\n",
            "Minibatch loss at step 5500: 0.462117\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 90.1%\n",
            "Minibatch loss at step 6000: 0.403120\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.1%\n",
            "Minibatch loss at step 6500: 0.482609\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 7000: 0.501831\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 7500: 0.537583\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 90.1%\n",
            "Minibatch loss at step 8000: 0.406768\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 8500: 0.400070\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 9000: 0.352553\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.4%\n",
            "Test accuracy: 95.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZy1QbB2Qoc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is getting really good. Let's try one layer deeper with dropouts."
      ]
    },
    {
      "metadata": {
        "id": "OelbNCwgQoc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_hidden_nodes1 = 1024\n",
        "num_hidden_nodes2 = 256\n",
        "num_hidden_nodes3 = 128\n",
        "keep_prob = 0.5\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  global_step = tf.Variable(0)\n",
        "\n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "        [image_size * image_size, num_hidden_nodes1],\n",
        "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
        "    )\n",
        "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
        "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
        "  weights3 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
        "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
        "  weights4 = tf.Variable(\n",
        "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
        "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
        "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
        "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
        "  \n",
        "  # Optimizer.\n",
        "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
        "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
        "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
        "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
        "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
        "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
        "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hEA5aH1Qoc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1938
        },
        "outputId": "49a2c567-19c8-4c58-970a-6e9d063ac304"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 18001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.initialize_all_variables().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 2.361296\n",
            "Minibatch accuracy: 5.5%\n",
            "Validation accuracy: 15.5%\n",
            "Minibatch loss at step 500: 0.293121\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 85.9%\n",
            "Minibatch loss at step 1000: 0.419165\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 86.5%\n",
            "Minibatch loss at step 1500: 0.397344\n",
            "Minibatch accuracy: 85.2%\n",
            "Validation accuracy: 88.7%\n",
            "Minibatch loss at step 2000: 0.190907\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 88.9%\n",
            "Minibatch loss at step 2500: 0.345336\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 88.8%\n",
            "Minibatch loss at step 3000: 0.289852\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 89.3%\n",
            "Minibatch loss at step 3500: 0.278356\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 4000: 0.272483\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 89.6%\n",
            "Minibatch loss at step 4500: 0.172843\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 5000: 0.349520\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 5500: 0.202181\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 6000: 0.221562\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 6500: 0.202083\n",
            "Minibatch accuracy: 93.0%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 7000: 0.287967\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 7500: 0.245889\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 8000: 0.195389\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 8500: 0.148186\n",
            "Minibatch accuracy: 94.5%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 9000: 0.121192\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 9500: 0.115484\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 10000: 0.021487\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 10500: 0.181479\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 11000: 0.123272\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 11500: 0.111949\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 91.3%\n",
            "Minibatch loss at step 12000: 0.144303\n",
            "Minibatch accuracy: 96.1%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 12500: 0.115403\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 91.1%\n",
            "Minibatch loss at step 13000: 0.036347\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.3%\n",
            "Minibatch loss at step 13500: 0.071117\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 91.3%\n",
            "Minibatch loss at step 14000: 0.043537\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 14500: 0.075257\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 15000: 0.068262\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 15500: 0.083675\n",
            "Minibatch accuracy: 97.7%\n",
            "Validation accuracy: 91.5%\n",
            "Minibatch loss at step 16000: 0.044764\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 91.1%\n",
            "Minibatch loss at step 16500: 0.016406\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.4%\n",
            "Minibatch loss at step 17000: 0.035720\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.5%\n",
            "Minibatch loss at step 17500: 0.048923\n",
            "Minibatch accuracy: 99.2%\n",
            "Validation accuracy: 91.5%\n",
            "Minibatch loss at step 18000: 0.015570\n",
            "Minibatch accuracy: 100.0%\n",
            "Validation accuracy: 91.3%\n",
            "Test accuracy: 96.1%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQQXCipQQodA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
      ]
    }
  ]
}